# Web Scraper

This project was built as a collaboration of myself and [Henry Kim](https://github.com/6henrykim).

* [Code](https://github.com/Hikaito/NLP_Portfolio/blob/main/Project_5/web_crawler.py):
Web Scraper built with Python.
* [Usage Report](https://github.com/Hikaito/NLP_Portfolio/blob/main/Project_5/Web_Crawler_Report.pdf):
Example of use case for the toy corpus generated by the code.

This project performs a web scrape of information from pages linked on a given initial page.

## Use
This project uses the `BeautifulSoup4` package and the `requests` package in addition to `nltk` and other packages.

The top of the project has a few global variables:
* `ROBOTS_FILES` stores robots.txt files during operation when robots.txt validation is used.
* `BANNED_BASE_URL` represents blacklisted base url's used to screen any url's scraped.
* `DICT_FILE` represents the filename for the file dictionary generated by the web scraping process.
* `CHECK_ROBOT` stores a boolean indicating whether to manually consult the `robots.txt` file of a website (true) or not (false).


## Functions
* `contains_keywords` checks whether a given text string contains keywords given in a list (true) or not (false).
* `approve_scrape` is used to incorporate manual approval or rejection of a robots.txt file for a website's url.
* `is_not_banned_url` is used to approve (true) or reject (false) a url based on the global list of banned url's. The `silent` parameter prints rejection notices when false (the default state).
* `gather_urls` is a function that scrapes the outgoing url's from a given url. The function accepts a list of keywords to favor and a list of filters to reject url's based on keywords; the `validate` parameter toggles type-checking for the inputs to the function.
* `scrape_raw_text` collects the text from a given url and writes the raw text to a file given in a parameter; this function uses the global `CHECK_ROBOT` variable to toggle robots.txt validation.
* `clean_text` is a function that reads text from a file, scrubs whitespace and other unimportant information from the text, and writes to a given file name.
* `scrape_and_clean` is a function that in a loop fetches and cleans the data from a list of url's using the `scrape_raw_text` and `clean_text` methods. The default filenames are `raw_text_{NUM}.txt` and `sentences_{NUM}.txt`.
* `tf-ids` accepts a list of file names and returns a tf-ids metric sorted list of keywords from greatest to least score.
* `scrape` is a driver function for the process of scraping data from one url's page and linked pages; returns a dictionary storing url's and associated output text.
* `most_frequent_terms`accepts a list of file names and returns a term frequency metric sorted list of keywords from greatest to least score.
* `create_database` generates a database from a given set of files and keywords, storing the database file in a given string. This function does not perform typechecking.
* The bottom of the file contains a 'main' program that executes an example usage of these functions.
