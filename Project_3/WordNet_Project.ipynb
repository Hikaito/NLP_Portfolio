{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WordNet Project\n",
        "\n",
        "WordNet is a lexical database of English words that links nouns, verbs, and adverbs into synsets that represent different concepts. Each synset collects different words that have the same meaning. WordNet organizes words with a heirarchical structure using hyponyms, homonyms, troponyms, and other relations to give synsets relative relationships with one another. WordNet was originally designed to model how the human mind stores word concepts."
      ],
      "metadata": {
        "id": "AyEhfj8x9BmT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "a6pC8pmyoioj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kGpAIAoZ85NZ",
        "outputId": "324a7928-f4da-43dc-c699-13ccf8293398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/webtext.zip.\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data] Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('sentiwordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "nltk.download('webtext')\n",
        "nltk.download('treebank')\n",
        "nltk.download('nps_chat')\n",
        "nltk.download('inaugural')\n",
        "nltk.download('genesis')\n",
        "nltk.download('gutenberg')\n",
        "from nltk.corpus import wordnet as wordnet\n",
        "from nltk.corpus import sentiwordnet as sentiwordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.wsd import lesk\n",
        "from nltk.book import text4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synset Interconnection for Nouns\n",
        "\n",
        "Nouns are organized with several relations between different synsets. A synset's hypernyms and hyponyms have an 'is a' relationship with the synset for nouns. Hypernyms are more overarching concepts whereas a hyponym narrows down a synset's concept to something more specific. Holonyms and Meronyms have an 'is a part of' relationship with a noun synset. A meronym is a smaller part of a synset's concept, whereas a holonym is a concept where the synset's concept is itself a component to a larger whole.\n",
        "\n",
        "Antonyms are accessed through lemmas."
      ],
      "metadata": {
        "id": "f7AcCxF1-JQF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore the synset of a noun\n",
        "Retrieve the synset for a given noun and display the definition, use examples, and lemmas of the first synset"
      ],
      "metadata": {
        "id": "G-bkBvmzoxfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve a synset group\n",
        "synsets = wordnet.synsets(\"vertebrae\", pos=wordnet.NOUN)\n",
        "print(synsets)\n",
        "\n",
        "# extract info of first synset, if there is a synset\n",
        "if synsets:\n",
        "  print(\"\\nSynset {}\\nDefinition: {}\\nUseage Examples: {}\\nLemmas: {}\".format(synsets[0],synsets[0].definition(), synsets[0].examples(), synsets[0].lemmas()))\n",
        "else:\n",
        "  print(\"No sysnet found for that\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6-Z8Ntbro55G",
        "outputId": "17ea18f4-51ff-463a-dadd-1acad85ab7b7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('vertebra.n.01')]\n",
            "\n",
            "Synset Synset('vertebra.n.01')\n",
            "Definition: one of the bony segments of the spinal column\n",
            "Useage Examples: []\n",
            "Lemmas: [Lemma('vertebra.n.01.vertebra')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the definition, useage, and lemmas for each synset of the word.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JjUf-TJIwnL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract info of all synsets, if there is a synset\n",
        "if synsets:\n",
        "  for syn in synsets:\n",
        "    print(\"\\nSynset {}\\nDefinition: {}\\nUseage Examples: {}\\nLemmas: {}\".format(syn,syn.definition(), syn.examples(), syn.lemmas()))\n",
        "else:\n",
        "  print(\"No synsets generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Ua-a9Rcawujy",
        "outputId": "5275916a-739b-4992-a08d-ddba794f8019"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synset Synset('vertebra.n.01')\n",
            "Definition: one of the bony segments of the spinal column\n",
            "Useage Examples: []\n",
            "Lemmas: [Lemma('vertebra.n.01.vertebra')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traverse the Tree of the first synset\n",
        "Traverse up the heirarchy from hyponyms as far as the hypernyms go, and then down as far as hyponyms go\n"
      ],
      "metadata": {
        "id": "CTnS_-EXyLEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if synsets:\n",
        "  # select the first synset\n",
        "  syn = synsets[0]\n",
        "\n",
        "  print(\"Word: {}\".format(syn))\n",
        "\n",
        "  # get hypernyms--------------------\n",
        "  hyp = syn.hypernyms()\n",
        "\n",
        "  # print hypernyms if existant\n",
        "  if hyp:\n",
        "    count = 1\n",
        "    while hyp:\n",
        "      print(\"Round {} hypernyms: {}\".format(count, hyp))\n",
        "      hyp = hyp[0].hypernyms()\n",
        "      count+=1\n",
        "\n",
        "  # print error if no hyponyms\n",
        "  else:\n",
        "    print(\"No hypernyms for {}\".format(syn))\n",
        "\n",
        "  # get hyponnyms--------------------\n",
        "  hyp = syn.hyponyms()\n",
        "\n",
        "  # print hyponyms if existant\n",
        "  if hyp:\n",
        "    count = 1\n",
        "    while hyp:\n",
        "      print(\"Round {} hyponyms: {}\".format(count, hyp))\n",
        "      hyp = hyp[0].hyponyms()\n",
        "      count+=1\n",
        "\n",
        "  # print error if no hyponyms\n",
        "  else:\n",
        "    print(\"No hyponyms for {}\".format(syn))\n",
        "else:\n",
        "  print(\"No synsets generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Pjr3Ue8yKhP",
        "outputId": "5b6d0a6c-f38d-4ae2-a75f-85f2c048755a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: Synset('vertebra.n.01')\n",
            "Round 1 hypernyms: [Synset('bone.n.01')]\n",
            "Round 2 hypernyms: [Synset('connective_tissue.n.01')]\n",
            "Round 3 hypernyms: [Synset('animal_tissue.n.01')]\n",
            "Round 4 hypernyms: [Synset('tissue.n.01')]\n",
            "Round 5 hypernyms: [Synset('body_part.n.01')]\n",
            "Round 6 hypernyms: [Synset('part.n.03')]\n",
            "Round 7 hypernyms: [Synset('thing.n.12')]\n",
            "Round 8 hypernyms: [Synset('physical_entity.n.01')]\n",
            "Round 9 hypernyms: [Synset('entity.n.01')]\n",
            "Round 1 hyponyms: [Synset('cervical_vertebra.n.01'), Synset('coccygeal_vertebra.n.01'), Synset('lumbar_vertebra.n.01'), Synset('sacral_vertebra.n.01'), Synset('thoracic_vertebra.n.01')]\n",
            "Round 2 hyponyms: [Synset('atlas.n.03'), Synset('axis.n.05')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Relations to a Synset\n",
        "Print the hypernyms, hyponyms, meronyms, holonyms, and antonyms of a selected synset.\n"
      ],
      "metadata": {
        "id": "AgxCXmoz8N34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# perform action if synset exists\n",
        "if synsets:\n",
        "  # select the first synset\n",
        "  syn = synsets[0]\n",
        "\n",
        "  # print metrics\n",
        "  print(\"\\nSynset {}\\nHypernyms: {}\\nHyponyms: {}\\nHolonyms: {}\\nMeronyms: {}\\nAntonyms: {}\".format(\n",
        "      syn,syn.hypernyms(), syn.hyponyms(), syn.part_holonyms(), syn.part_meronyms(), syn.lemmas()[0].antonyms()))\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"No synsets generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_9yF8G2G_ANQ",
        "outputId": "165e75fd-34f5-4ee6-e466-cb11e96d4110"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Synset Synset('vertebra.n.01')\n",
            "Hypernyms: [Synset('bone.n.01')]\n",
            "Hyponyms: [Synset('cervical_vertebra.n.01'), Synset('coccygeal_vertebra.n.01'), Synset('lumbar_vertebra.n.01'), Synset('sacral_vertebra.n.01'), Synset('thoracic_vertebra.n.01')]\n",
            "Holonyms: [Synset('spinal_column.n.01')]\n",
            "Meronyms: [Synset('apophysis.n.02'), Synset('centrum.n.01'), Synset('transverse_process.n.01')]\n",
            "Antonyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Synset Interconnection for Verbs\n",
        "\n",
        "The arrangement of Verbs has a similar structure to that of nouns.\n",
        "* Hypernyms are less specific ways of describing the same action.\n",
        "* Hyponyms are more specific ways of describing the same action.\n",
        "* Antonyms describe the opposite of the action, if the action has a clear inverse (ex. sitting and rising)\n",
        "* Holonyms and Meronyms are more difficult to associate with verbs because verbs are rarely 'part of a whole'."
      ],
      "metadata": {
        "id": "rsKNk4xV-ObY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a verb\n",
        "synsets = wordnet.synsets(\"soar\", pos=wordnet.VERB)\n",
        "print(synsets)\n",
        "\n",
        "# extract info of first synset, if there is a synset\n",
        "if synsets:\n",
        "  print(\"\\nSynset {}\\nDefinition: {}\\nUseage Examples: {}\\nLemmas: {}\".format(synsets[0],synsets[0].definition(), synsets[0].examples(), synsets[0].lemmas()))\n",
        "else:\n",
        "  print(\"No sysnet found for that\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AHT8oR-XG_xA",
        "outputId": "d56471fb-422b-4d47-89bc-50566d7a15ff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('soar.v.01'), Synset('hang_glide.v.01'), Synset('soar.v.03'), Synset('soar.v.04'), Synset('sailplane.v.01')]\n",
            "\n",
            "Synset Synset('soar.v.01')\n",
            "Definition: rise rapidly\n",
            "Useage Examples: ['the dollar soared against the yen']\n",
            "Lemmas: [Lemma('soar.v.01.soar'), Lemma('soar.v.01.soar_up'), Lemma('soar.v.01.soar_upwards'), Lemma('soar.v.01.surge'), Lemma('soar.v.01.zoom')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Traverse Up the Tree of the first synset\n",
        "Traverse up the heirarchy from hyponyms as far as the hypernyms go, and then down as far as hyponyms go\n"
      ],
      "metadata": {
        "id": "E5_W-0wBG6at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if synsets:\n",
        "  # select the first synset\n",
        "  syn = synsets[0]\n",
        "\n",
        "  print(\"Word: {}\".format(syn))\n",
        "\n",
        "  # get hypernyms--------------------\n",
        "  hyp = syn.hypernyms()\n",
        "\n",
        "  # print hypernyms if existant\n",
        "  if hyp:\n",
        "    count = 1\n",
        "    while hyp:\n",
        "      print(\"Round {} hypernyms: {}\".format(count, hyp))\n",
        "      hyp = hyp[0].hypernyms()\n",
        "      count+=1\n",
        "\n",
        "  # print error if no hyponyms\n",
        "  else:\n",
        "    print(\"No hypernyms for {}\".format(syn))\n",
        "\n",
        "  # get hyponnyms--------------------\n",
        "  hyp = syn.hyponyms()\n",
        "\n",
        "  # print hyponyms if existant\n",
        "  if hyp:\n",
        "    count = 1\n",
        "    while hyp:\n",
        "      print(\"Round {} hyponyms: {}\".format(count, hyp))\n",
        "      hyp = hyp[0].hyponyms()\n",
        "      count+=1\n",
        "\n",
        "  # print error if no hyponyms\n",
        "  else:\n",
        "    print(\"No hyponyms for {}\".format(syn))\n",
        "else:\n",
        "  print(\"No synsets generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "68950250-a576-4f99-d683-4117e6a191d4",
        "id": "wKcu-IB5HAGU"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: Synset('soar.v.01')\n",
            "Round 1 hypernyms: [Synset('rise.v.01')]\n",
            "Round 2 hypernyms: [Synset('travel.v.01')]\n",
            "Round 1 hyponyms: [Synset('billow.v.01')]\n",
            "Round 2 hyponyms: [Synset('cloud.v.03')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Morphologies\n",
        "Generate morphologies for the word. As you can see, all of these morphologies are accepted by the morphy function as valid forms of the word, but there is no such thing as 'wolfshes'. Morphy is a rules-based simplifier, and here you can see the limitations in that, as morphy accepts words that have no business being in the English language as valid morphologies to words.\n",
        "\n",
        "However, this does have some benefits for parsing 'Internet English', where morphologies of words are invented to convey self-expression.\n",
        "Example: \"Wow look at all them wolfies\" is not proper English but is representative of how some people talk in messages."
      ],
      "metadata": {
        "id": "JVpK9Wd0HM5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a word\n",
        "word = \"wolf\"\n",
        "\n",
        "# print metrics\n",
        "print(\"Word: {}\", word)\n",
        "\n",
        "noun = [\"s\",\"ses\",\"xes\",\"zes\",\"ches\",\"shes\",\"men\",\"ies\"]\n",
        "verb = [\"s\",\"ies\",\"es\",\"ed\",\"ing\"]\n",
        "adjactive = [\"er\",\"est\"]\n",
        "\n",
        "print(\"Accepted (by morphy) Variant forms:\")\n",
        "\n",
        "# generate nouns\n",
        "for suffix in noun:\n",
        "  if wordnet.morphy(word+\"s\", wordnet.NOUN) != None:\n",
        "    print(\"Noun: \", word + suffix)\n",
        "\n",
        "# generate nouns\n",
        "for suffix in verb:\n",
        "  if wordnet.morphy(word+\"s\", wordnet.VERB) != None:\n",
        "    print(\"Verb: \", word + suffix)\n",
        "\n",
        "# generate nouns\n",
        "for suffix in adjactive:\n",
        "  if wordnet.morphy(word+\"s\", wordnet.ADJ) != None:\n",
        "    print(\"Adjactive: \", word + suffix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MWwPAXtGHMF9",
        "outputId": "63625993-20c2-4826-94da-ac470b7a145f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: {} wolf\n",
            "Accepted (by morphy) Variant forms:\n",
            "Noun:  wolfs\n",
            "Noun:  wolfses\n",
            "Noun:  wolfxes\n",
            "Noun:  wolfzes\n",
            "Noun:  wolfches\n",
            "Noun:  wolfshes\n",
            "Noun:  wolfmen\n",
            "Noun:  wolfies\n",
            "Verb:  wolfs\n",
            "Verb:  wolfies\n",
            "Verb:  wolfes\n",
            "Verb:  wolfed\n",
            "Verb:  wolfing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity (Wu-Palmer and Lesk)\n"
      ],
      "metadata": {
        "id": "iNsxChGIzXG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select two words"
      ],
      "metadata": {
        "id": "hAOPmmk9EXIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# word\n",
        "word_a = \"wolf\"\n",
        "word_b = \"cat\"\n",
        "\n",
        "# use of word\n",
        "word_a_use = \"The wolf pack raced after the deer.\"\n",
        "word_b_use = \"Sometimes I like to give my cat extra treats.\"\n",
        "\n",
        "# possible synsets for both words\n",
        "synsa = wordnet.synsets(word_a)\n",
        "synsb = wordnet.synsets(word_b)\n",
        "\n",
        "# select the first synset (can be changed by the next step)\n",
        "syna = synsa[0]\n",
        "synb = synsb[0]"
      ],
      "metadata": {
        "id": "LJMHo93m__J1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the desired synset from the words (manual tuning)"
      ],
      "metadata": {
        "id": "CUf7V-4DEYzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract synset a\n",
        "syna = synsa[0]\n",
        "print(\"Synsets for a: {}\".format(synsa))\n",
        "\n",
        "print(\"\\nSynset {}\\nDefinition: {}\\nUseage Examples: {}\\nLemmas: {}\".format(syna,syna.definition(), syna.examples(), syna.lemmas()))\n",
        "\n",
        "# extract synset b\n",
        "synb = synsb[0]\n",
        "print(\"\\n\\nSynsets for b: {}\".format(synsb))\n",
        "\n",
        "print(\"\\nSynset {}\\nDefinition: {}\\nUseage Examples: {}\\nLemmas: {}\".format(synb,synb.definition(), synb.examples(), synb.lemmas()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OKMW2yk0Ee81",
        "outputId": "03dbf843-bbbf-4fba-c596-58ef616fde26"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synsets for a: [Synset('wolf.n.01'), Synset('wolf.n.02'), Synset('wolf.n.03'), Synset('wolf.n.04'), Synset('beast.n.02'), Synset('wolf.v.01')]\n",
            "\n",
            "Synset Synset('wolf.n.01')\n",
            "Definition: any of various predatory carnivorous canine mammals of North America and Eurasia that usually hunt in packs\n",
            "Useage Examples: []\n",
            "Lemmas: [Lemma('wolf.n.01.wolf')]\n",
            "\n",
            "\n",
            "Synsets for b: [Synset('cat.n.01'), Synset('guy.n.01'), Synset('cat.n.03'), Synset('kat.n.01'), Synset('cat-o'-nine-tails.n.01'), Synset('caterpillar.n.02'), Synset('big_cat.n.01'), Synset('computerized_tomography.n.01'), Synset('cat.v.01'), Synset('vomit.v.01')]\n",
            "\n",
            "Synset Synset('cat.n.01')\n",
            "Definition: feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
            "Useage Examples: []\n",
            "Lemmas: [Lemma('cat.n.01.cat'), Lemma('cat.n.01.true_cat')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Wu-Palmer Similarty\n",
        "The Wu-Palmer algorithm determines a score of 'similarity' from common ancestor words shared between the two words tested, whereas the Path Similarity metric compares how similar the paths to the words are.\n",
        "* Both range from 0 (no similarity) to 1 (identity)\n",
        "* The Wu-Palmer algorithm calculates similarity from the distances of hypernym levels to the common ancestor.\n",
        "* The path similarity score is a more brute-force measurement of similarity between paths.\n",
        "* The Wu-Palmer algorithm tends to have a higher score than the path-similarity algorithm."
      ],
      "metadata": {
        "id": "45wAL71CFZc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Wu-Palmer score: {}\".format(wordnet.wup_similarity(syna, synb)))\n",
        "print(\"Path-Similarity score: {}\".format(syna.path_similarity(synb)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1eY9UjR8FdZE",
        "outputId": "30b8a7c8-b46a-4a35-d973-5fb9a4b7ea62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wu-Palmer score: 0.8571428571428571\n",
            "Path-Similarity score: 0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lesk Algorithm\n",
        "The Lesk algorithm is used for identifying what synset a given word most likely belongs to. It tends to correctly guage if the word is a noun, verb, or adjactive, but it fails with more complex context. \n",
        "For example:\n",
        "* \"The wolf pack raced after the deer.\" - identifies 'wolf' as a german classical scholar. \n",
        "* \"Sometimes I like to give my cat extra treats.\" - identifies 'cat' as 'kat', a type of plant used as a drug. \n",
        "\n",
        "Using Lesk with a sentence whose stopwords have been removed appears to improve performance significantly (see SentiWordNet example for demonstration)."
      ],
      "metadata": {
        "id": "Vfxl8YRZa6Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize examples\n",
        "tokens_a = word_tokenize(word_a_use)\n",
        "tokens_b = word_tokenize(word_b_use)\n",
        "\n",
        "# lesk algorithm\n",
        "lesk_a = lesk(tokens_a, word_a)\n",
        "lesk_b = lesk(tokens_b, word_b)\n",
        "print(\"Example sentence tokens: \", tokens_a)\n",
        "print(\"Lesk Synset for {}: {} - {}\".format(word_a, lesk_a, lesk_a.definition()))\n",
        "print(\"Example sentence tokens: \", tokens_b)\n",
        "print(\"Lesk Synset for {}: {} - {}\".format(word_b, lesk_b, lesk_b.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IgtCCqJSa_i3",
        "outputId": "24e064c8-d306-4549-a42a-4c2a19526570"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example sentence tokens:  ['The', 'wolf', 'pack', 'raced', 'after', 'the', 'deer', '.']\n",
            "Lesk Synset for wolf: Synset('wolf.n.03') - German classical scholar who claimed that the Iliad and Odyssey were composed by several authors (1759-1824)\n",
            "Example sentence tokens:  ['Sometimes', 'I', 'like', 'to', 'give', 'my', 'cat', 'extra', 'treats', '.']\n",
            "Lesk Synset for cat: Synset('kat.n.01') - the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SentiWordNet\n",
        "SentiWordNet is a tool that analyzes the sentiment in words. It can be used in applications that measure the emotional intent behind words, such as detecting hate speech in social media posts, detecting depression in conversation records, or allowing an artificial conversation to correctly identify the tone behind words and appropriately respond to the sentiments expressed by a human user.\n",
        "\n",
        "SentiWordNet can analyse the sentiment of entire sentences though a function of the independent sentiments of the words. The example given here removes stopwords and then assigns specific synsets to tokens using Lesk before collecting the sum of positive and negative sentiments across the tokens to assign a sentiment metric to the entire sentence."
      ],
      "metadata": {
        "id": "gIDXNHUhn9pm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select a word\n",
        "word = \"terrific\"\n",
        "syns = wordnet.synsets(word)\n",
        "print(\"Possible synsets: \", syns)\n",
        "syn = syns[1]\n",
        "print(\"Synset {}\\nDefinition: {}\".format(syn,syn.definition()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BjSVUMe6olEh",
        "outputId": "9841fc5b-13a9-4865-dcbe-b75698624417"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Possible synsets:  [Synset('terrific.s.01'), Synset('fantastic.s.02'), Synset('terrific.s.03')]\n",
            "Synset Synset('fantastic.s.02')\n",
            "Definition: extraordinarily good or great ; used especially as intensifiers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# score the word sentiment\n",
        "sents = sentiwordnet.senti_synsets(word)\n",
        "for sent in sents:\n",
        "  print(\"{} - Definition: {}\".format(sent.synset,sent.synset.definition()))\n",
        "  print(\"Positive Score: {}\\nNegative score: {}\\nObjective score: {}\".format(sent.pos_score(), sent.neg_score(), sent.obj_score()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ymjjtIuXp_-O",
        "outputId": "c487c580-d0cc-4bd6-e240-0bc91a6a207b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('terrific.s.01') - Definition: very great or intense\n",
            "Positive Score: 0.25\n",
            "Negative score: 0.25\n",
            "Objective score: 0.5\n",
            "Synset('fantastic.s.02') - Definition: extraordinarily good or great ; used especially as intensifiers\n",
            "Positive Score: 0.75\n",
            "Negative score: 0.0\n",
            "Objective score: 0.25\n",
            "Synset('terrific.s.03') - Definition: causing extreme terror\n",
            "Positive Score: 0.0\n",
            "Negative score: 0.625\n",
            "Objective score: 0.375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyze the sentiment of a sentence, using Lesk to identify the synset for a token. Removing the stopwords significantly improves the functionality of Lesk, as Lesk will assign incorrect tokens to stop words (such as 'a' = 'an amino acit in deoxyriboneucleic acid'). With false assignments absent, the remaining words are assigned much more accurate synsets to their intended interpretation."
      ],
      "metadata": {
        "id": "960wZSzDtqvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# score the sentiment of a whole sentence of words\n",
        "sentence = \"When I look out at the moon, I imagine you are looking up at the same moon as I.\" # mostly negative\n",
        "sentence = \"Together we will walk through a somber woods, hand in hand, your shadow swallowed by mine in the calm morning sunlight.\" # partly positive but mostly negative\n",
        "sentence = \"Such a maddening delight, for power to corrupt my veins as surely as fire consuming dry paper.\" # mostly positive\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "# remove stopwords\n",
        "stopset = stopwords.words('english')\n",
        "tokens = [token for token in tokens if token not in stopset]\n",
        "\n",
        "positive = 0\n",
        "negative = 0\n",
        "# gather scores\n",
        "for token in tokens:\n",
        "  syn = lesk(tokens, token)\n",
        "  if syn:\n",
        "    sent = sentiwordnet.senti_synset(syn.name())\n",
        "    print(\"Word {} ({})\\nPos: {}, Neg: {}, Obj: {}\".format(syn.name(), syn.definition(), sent.pos_score(), sent.neg_score(), sent.obj_score()))\n",
        "    positive += sent.pos_score()\n",
        "    negative += sent.neg_score()\n",
        "    \n",
        "print(\"\\nPositive score total: {}\\nNegative score total: {}\".format(positive, negative))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "XK6hRPz4s2cR",
        "outputId": "66346371-4b99-474c-9c5b-4c21c2e18f82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word such.s.01 (of so extreme a degree or extent)\n",
            "Pos: 0.0, Neg: 0.125, Obj: 0.875\n",
            "Word madden.v.03 (make mad)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word delight.v.02 (take delight in)\n",
            "Pos: 0.0, Neg: 0.25, Obj: 0.75\n",
            "Word power.v.01 (supply the force or power for the functioning of)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word corrupt.v.01 (corrupt morally or by intemperance or sensuality)\n",
            "Pos: 0.0, Neg: 0.625, Obj: 0.375\n",
            "Word vein.v.01 (make a veinlike pattern)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word surely.r.01 (definitely or positively (`sure' is sometimes used informally for `surely'))\n",
            "Pos: 0.25, Neg: 0.0, Obj: 0.75\n",
            "Word fire.v.06 (drive out or away by or as if by fire)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word devour.v.03 (eat immoderately)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word dry.v.02 (become dry or drier)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "Word paper.v.01 (cover with paper)\n",
            "Pos: 0.0, Neg: 0.0, Obj: 1.0\n",
            "\n",
            "Positive score total: 0.25\n",
            "Negative score total: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations\n",
        "Collocations are groups of words that together have a different meaning than the words when taken individually. The words in a collocation cannot be swapped for synonyms without changing the meaning of the collocation.\n",
        "\n",
        " A good example of this is 'dead ahead'. On their own, the words 'dead' and 'ahead' imply that there are zombies in front of us, whereas 'dead ahead' when taken as a single unit means only directly ahead. This is an example of a collocation. To say 'deceased ahead' would change the meaning entirelly.\n",
        "\n",
        "Collocations can be detected in text when words appear next to eachother with an unusually high probability."
      ],
      "metadata": {
        "id": "sHtmlGdG2zxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect collocations\n",
        "col = text4.collocations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SkLOhRBw5z9q",
        "outputId": "21c19217-de88-42b4-e76c-5e6bc085fbe3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "United States; fellow citizens; years ago; four years; Federal\n",
            "Government; General Government; American people; Vice President; God\n",
            "bless; Chief Justice; one another; fellow Americans; Old World;\n",
            "Almighty God; Fellow citizens; Chief Magistrate; every citizen; Indian\n",
            "tribes; public debt; foreign nations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate mutual information for the first collocation\n",
        "\n",
        "Mutual information is a metric calcualted as:\n",
        "\n",
        "log(P(x,y) / (P(x) * P(Y)))\n",
        "\n",
        "The mutual information score is the log of the probability that the two words appeared together instead divided by the ordinary probability of appearing together together own.\n",
        "\n",
        "The mutual information formula is useful, but could be made futher specialized by checking probabilities only among different word groups that have the same part of speech (noun, verb, etc) since certain groups of words may have a higher probability of appearing by one another in a sequence. Furthermore, whether or not a group of words appears as a collorary could be influenced by both the corpus and the score"
      ],
      "metadata": {
        "id": "aRwpCvzU5zov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# select collocations\n",
        "a = \"God\"\n",
        "b = \"bless\"\n",
        "ab = a + \" \" + b\n",
        "\n",
        "word_count = len(list(text4))\n",
        "text = ' '.join(text4.tokens) # recreate text\n",
        "\n",
        "# calculate probabilities for each word\n",
        "p_a = text.count(a) / word_count\n",
        "p_b = text.count(b) / word_count\n",
        "\n",
        "# calculate probability of words together\n",
        "p_ab = text.count(ab) / word_count\n",
        "\n",
        "# calculate and scorePMI\n",
        "pmi = math.log2(p_ab / (p_a * p_b))\n",
        "\n",
        "print(\"PMI score: \", pmi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "W8cEh8FLK6zZ",
        "outputId": "cee32fb0-e14f-461f-c091-90f8abe16a04"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PMI score:  8.076081481141687\n"
          ]
        }
      ]
    }
  ]
}