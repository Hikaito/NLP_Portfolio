{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TqSjKu37vUrG",
        "TNpqS_Sx8-7g",
        "sogmjbhvM4Wm"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification\n"
      ],
      "metadata": {
        "id": "7DajgzG2rz3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code by Hikaito"
      ],
      "metadata": {
        "id": "-ZUBpphESOoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Collect the Dataset\n",
        "This project uses books sourced from Project Gutenburg.\n",
        "* Mary Shelly's Frankenstein,\n",
        "* Herman Melville's Moby Dick"
      ],
      "metadata": {
        "id": "TqSjKu37vUrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib import request\n",
        "# please note that the /cache folder of project gutenburg is discouraged by robots.txt\n",
        "#   therefore use discretion for pulling from url1 as it may not exist\n",
        "#   and may get you in trouble.\n",
        "url1 = \"https://www.gutenberg.org/cache/epub/84/pg84.txt\"\n",
        "book1_title = \"Frankenstein, by Mary Wollstonecraft Shelley\"\n",
        "url2 = \"https://www.gutenberg.org/files/2701/2701-0.txt\"\n",
        "book2_title = \"Moby Dick, by Herman Melville\"\n",
        "\n",
        "book_names = [book1_title, book2_title]"
      ],
      "metadata": {
        "id": "sAPMzCEKxC-V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Text\n",
        "Import the raw text of each book in the dataset as a string.\n",
        "\n",
        "At the time of writing, Project Gutenburg (the source) recommends a crawl delay of 5 seconds between requests."
      ],
      "metadata": {
        "id": "LTpIk5UuzY33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with request.urlopen(url1) as f:\n",
        "    raw1 = f.read().decode('utf-8-sig')\n",
        "\n",
        "print(raw1[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQGghFJGy9JK",
        "outputId": "ba77792b-6884-4b13-f4a3-5704773ac71d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of Frankenstein, by Mary Wollstonecraft Shelley\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with request.urlopen(url2) as f:\n",
        "    raw2 = f.read().decode('utf-8-sig')\n",
        "\n",
        "print(raw2[:200])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB6cw6tIzgcr",
        "outputId": "2b1c17cf-4c76-4257-9350-c65b807c2d42"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of Moby-Dick; or The Whale, by Herman Melville\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "most other parts of the world at no cost and with a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parse Sentences\n",
        "This step builds two datasets of sentences in text, removing the introductory text and the legal footer from the raw text.\n",
        "\n",
        "The exact location of the correct start and end sentences are manually examined; the sentences that are printed are the last discarded sentence at the front of a corpus and the first discarded sentence ad the end of a corpus.\n",
        "\n",
        "The legal header and footer are almost the same across both texts and are not part of the original work, so these are ommitted."
      ],
      "metadata": {
        "id": "PM_VS0vwb-T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse into sentences\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "sentence = [0,0]\n",
        "sentence[0] = sent_tokenize(raw1)\n",
        "sentence[1] = sent_tokenize(raw2)\n",
        "\n",
        "# trim off licensing at top\n",
        "apparentIntro1 = 6# manually set to skip the introductory text\n",
        "apparentIntro2 = 4# manually set to skip the introductory text\n",
        "print(sentence[0][apparentIntro1])\n",
        "print(sentence[1][apparentIntro2])\n",
        "\n",
        "# trim off licensing at bottom\n",
        "# 108 for lowercase\n",
        "apparentOutro1 = len(sentence[0])-115 # manually set to skip the introductory text\n",
        "apparentOutro2 = len(sentence[1])-115 # manually set to skip the introductory text\n",
        "print(sentence[0][apparentOutro1])\n",
        "print(sentence[1][apparentOutro2])\n",
        "\n",
        "sentence[0] = sentence[0][apparentIntro1 + 1:apparentOutro1 - 1]\n",
        "sentence[1] = sentence[1][apparentIntro2 + 1:apparentOutro2 - 1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3s1m2uyzqBH",
        "outputId": "96a3ddb4-d582-43b8-91da-433cd8c05120"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** START OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Frankenstein;\r\n",
            "\r\n",
            "or, the Modern Prometheus\r\n",
            "\r\n",
            "by Mary Wollstonecraft (Godwin) Shelley\r\n",
            "\r\n",
            "\r\n",
            " CONTENTS\r\n",
            "\r\n",
            " Letter 1\r\n",
            " Letter 2\r\n",
            " Letter 3\r\n",
            " Letter 4\r\n",
            " Chapter 1\r\n",
            " Chapter 2\r\n",
            " Chapter 3\r\n",
            " Chapter 4\r\n",
            " Chapter 5\r\n",
            " Chapter 6\r\n",
            " Chapter 7\r\n",
            " Chapter 8\r\n",
            " Chapter 9\r\n",
            " Chapter 10\r\n",
            " Chapter 11\r\n",
            " Chapter 12\r\n",
            " Chapter 13\r\n",
            " Chapter 14\r\n",
            " Chapter 15\r\n",
            " Chapter 16\r\n",
            " Chapter 17\r\n",
            " Chapter 18\r\n",
            " Chapter 19\r\n",
            " Chapter 20\r\n",
            " Chapter 21\r\n",
            " Chapter 22\r\n",
            " Chapter 23\r\n",
            " Chapter 24\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "Letter 1\r\n",
            "\r\n",
            "_To Mrs. Saville, England._\r\n",
            "\r\n",
            "\r\n",
            "St. Petersburgh, Dec. 11th, 17—.\n",
            "By Herman Melville\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "CONTENTS\r\n",
            "\r\n",
            "ETYMOLOGY.\n",
            "*** END OF THE PROJECT GUTENBERG EBOOK FRANKENSTEIN ***\r\n",
            "\r\n",
            "Updated editions will replace the previous one--the old editions will\r\n",
            "be renamed.\n",
            "*** END OF THE PROJECT GUTENBERG EBOOK MOBY-DICK; OR THE WHALE ***\r\n",
            "\r\n",
            "***** This file should be named 2701-0.txt or 2701-0.zip *****\r\n",
            "This and all associated files of various formats will be found in:\r\n",
            "    https://www.gutenberg.org/2/7/0/2701/\r\n",
            "\r\n",
            "Updated editions will replace the previous one--the old editions will\r\n",
            "be renamed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataframe of text\n",
        "Assign each sentence a classifier and then shuffle a dataframe with both book sentences inside"
      ],
      "metadata": {
        "id": "b4-8BZlB4Pkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a dataframe for each book\n",
        "import pandas as pd\n",
        "classifier = [0] * len(sentence[0])\n",
        "intext = {'Sentence': sentence[0], 'Book': classifier}\n",
        "book1 = pd.DataFrame(data=intext)\n",
        "\n",
        "classifier = [1] * len(sentence[1])\n",
        "intext = {'Sentence': sentence[1], 'Book': classifier}\n",
        "book2 = pd.DataFrame(data=intext)\n",
        "\n",
        "# create a joint dataframe and scramble it\n",
        "books = pd.concat([book1, book2])  # create a joined dataframe\n",
        "print(books[:10], books[-10:])  # prints the text at the top and bottom to double check\n",
        "booksShuffle = books.sample(frac=1)\n",
        "print(booksShuffle[:10], booksShuffle[-10:])  # prints the text at the top and bottom to double check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TA-LOxGM5Dwc",
        "outputId": "a8e00938-c809-4349-dd46-1db0e3018839"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                            Sentence  Book\n",
            "0  You will rejoice to hear that no disaster has ...     0\n",
            "1  I arrived here yesterday, and my first task is...     0\n",
            "2  I am already far north of London, and as I wal...     0\n",
            "3                 Do you understand this\\r\\nfeeling?     0\n",
            "4  This breeze, which has travelled from the regi...     0\n",
            "5  Inspirited by this wind of promise, my daydrea...     0\n",
            "6  I try in vain to be persuaded that the pole is...     0\n",
            "7  There, Margaret, the sun is for ever\\r\\nvisibl...     0\n",
            "8  There—for with your leave, my sister, I will p...     0\n",
            "9  Its productions and features may be without ex...     0                                                Sentence  Book\n",
            "9050                                  The drama’s done.     1\n",
            "9051  Why then here does any one step forth?—Because...     1\n",
            "9052  It so chanced, that after the Parsee’s disappe...     1\n",
            "9053  So,\\r\\nfloating on the margin of the ensuing s...     1\n",
            "9054  When I reached it, it had\\r\\nsubsided to a cre...     1\n",
            "9055  Round and round, then, and ever contracting\\r\\...     1\n",
            "9056  Till, gaining that\\r\\nvital centre, the black ...     1\n",
            "9057  Buoyed up by that coffin, for almost\\r\\none wh...     1\n",
            "9058  The\\r\\nunharming sharks, they glided by as if ...     1\n",
            "9059  On the second day, a\\r\\nsail drew near, nearer...     1\n",
            "                                               Sentence  Book\n",
            "7774                                                Oh!     1\n",
            "8541  “What d’ye see?” cried Ahab, flattening his fa...     1\n",
            "1182  Exhaustion succeeded\\r\\nto the extreme fatigue...     0\n",
            "2708                      Wherefore this\\r\\ndifference?     1\n",
            "953   If she\\r\\nis, as you believe, innocent, rely o...     0\n",
            "639       During all that time Henry was my only nurse.     0\n",
            "2451  Now, then, come the\\r\\ngrand divisions of the ...     1\n",
            "1761  I struggled vainly for\\r\\nfirmness sufficient ...     0\n",
            "4660                                          The Line.     1\n",
            "7898  Shake yourself; you’re\\r\\nAquarius, or the wat...     1                                                Sentence  Book\n",
            "3001                                                Ha!     1\n",
            "25                                          CHAPTER 13.     1\n",
            "1698                 The Lord be merciful to his ghost!     1\n",
            "1850  I felt emotions of gentleness and pleasure, th...     0\n",
            "1168  Ruined castles hanging on the precipices of pi...     0\n",
            "7573  But Death plucked\\r\\ndown some virtuous elder ...     1\n",
            "7914  Seems to me we are lashing down these anchors\\...     1\n",
            "8695  “Aye, aye!” cried Stubb, “I knew it—ye can’t e...     1\n",
            "1802  The inside\\r\\nof the cottage was dark, and I h...     0\n",
            "1193  Where had they fled when the next morning I aw...     0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Dataset Statistics\n",
        "This dataset features two books with the goal of classifying text as most likely originating from one book or the other. The dataset is assembled and cleaned by the web scraper provided in part 1.\n",
        "\n",
        "The model extrapolates to predicting which book text a user provides most resembles.\n",
        "\n",
        "Books are sourced from Project Gutenberg; I originally investigated Kaggle as a data source but was intrigued by a dataset that mentioned Project Gutenberg and decided to just develop my own dataset."
      ],
      "metadata": {
        "id": "TNpqS_Sx8-7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Divide to test and train\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(books.Sentence, books.Book, test_size=.3, random_state = 11)\n"
      ],
      "metadata": {
        "id": "8uR52U_S_GGE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create graphs\n",
        "import matplotlib.pyplot as plt\n",
        "# graph 1: overall size\n",
        "data = {'Book 1': len(sentence[0]), 'Book 2': len(sentence[1])}\n",
        "plot_y = list(data.values())\n",
        "plot_x = list(data.keys())\n",
        "\n",
        "# graph 2: count samples for training\n",
        "df_train = pd.DataFrame(data={'X': x_train, 'Y': y_train})\n",
        "df_train_count = df_train.pivot_table(columns=['Y'], aggfunc='size')\n",
        "\n",
        "data2 = {'Book 1': df_train_count[0], 'Book 2': df_train_count[1], 'Combined': x_train.size}\n",
        "plot_y2 = list(data2.values())\n",
        "plot_x2 = list(data2.keys())\n",
        "\n",
        "# graph 3: test\n",
        "df_test = pd.DataFrame(data={'X': x_test, 'Y': y_test})\n",
        "df_test_count = df_test.pivot_table(columns=['Y'], aggfunc='size')\n",
        "\n",
        "data3 = {'Book 1': df_test_count[0], 'Book 2': df_test_count[1], 'Combined': x_test.size}\n",
        "plot_y3 = list(data3.values())\n",
        "plot_x3 = list(data3.keys())\n",
        "\n",
        "\n",
        "# display graphs\n",
        "fig, ax = plt.subplots(1, 3, figsize = (9, 3), sharey = True)\n",
        "bar0 = ax[0].bar(plot_x, plot_y)\n",
        "ax[0].bar_label(bar0)\n",
        "bar1 = ax[1].bar(plot_x2, plot_y2)\n",
        "ax[1].bar_label(bar1)\n",
        "bar2 = ax[2].bar(plot_x3, plot_y3)\n",
        "ax[2].bar_label(bar2)\n",
        "ax[0].title.set_text(\"Total Text\")\n",
        "ax[1].title.set_text(\"Text for Train\")\n",
        "ax[2].title.set_text(\"Text for Test\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "6co05meuAHms",
        "outputId": "2d369970-4e44-4ad9-f216-74bca6190bba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x300 with 3 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAAEpCAYAAADiXYbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABKvElEQVR4nO3deXzM1/4/8Ndkm8Qkk02TSYgkpM2iSIQyghaRUIqflColVG1NuOFeSq8ltcUeisu1lNSlltbSS2Wxb5FoCKkQtIKSpS4SgpDM+f3hm09NkyBElk9ez8fj83iYc87nzDmfz7zHe04+8xmFEEKAiIiIiIhkxaCyB0BEREREROWPiT4RERERkQwx0SciIiIikiEm+kREREREMsREn4iIiIhIhpjoExERERHJEBN9IiIiIiIZYqJPRERERCRDTPSJiIiIiGSIiT5ViAMHDkChUODAgQOVPRQiegEnTpxAq1atoFKpoFAokJycXNlDeiXh4eFQKBSVPQyiKkVucU7FMdGXMYVC8ULbiyTfM2fOxPbt26vNeF/EjRs3EB4ezjc2qnBV/bX++PFj9OrVC7du3UJkZCTWrVsHZ2fnchnLX7m4uLzQsVi7du1reX6i14Vx/qeKjvOKyFmqC6PKHgC9PuvWrdN7/O233yIuLq5Yuaen53P7mjlzJj788EP06NGjPIeopzzH+yJu3LiBr776Ci4uLvD29i6XPoleRFV/rf/666+4cuUKVq5cic8++6xcxlCahQsX4t69e9Ljn376Cd999x0iIyNRu3ZtqbxVq1av9DwTJ07E+PHjX6kPorJgnP+pouK8SEXkLNUFE30Z++STT/QeHz9+HHFxccXKq4rqNl6il1XVX+vZ2dkAACsrq3LrMy8vDyqVqlj5X/8jzszMxHfffYcePXrAxcWlzP2VxsjICEZG/C+PKg7j/E8vG+f06njpTg2Xl5eHv//973BycoJSqYS7uzvmzZsHIYTURqFQIC8vD1FRUdKf1wYOHAgAuHLlCj7//HO4u7vDzMwMtra26NWrF9LT01/LeHU6HRYuXIiGDRvC1NQU9vb2GDZsGG7fvi21mTJlCgwMDLB37169fYcOHQoTExOcPn0aBw4cQPPmzQEAgwYN4uUBVOVU1mt94MCBePfddwEAvXr1gkKhwHvvvSfV79u3D23atIFKpYKVlRW6d++Oc+fO6fVRdD18amoq+vbtC2tra7Ru3fqlj8XAgQNhbm6OX3/9Fe+//z4sLCzQr18/AMDhw4fRq1cv1KtXD0qlEk5OThg9ejQePHhQ4pieplAoEBoaiu3bt+Ptt9+GUqlEw4YNER0d/dJjJSoLxrm+//znP/D19YWZmRlsbGzQp08fXLt2Ta/NxYsXERQUBI1GA1NTU9StWxd9+vRBTk4OgGfnLDURlzdqMCEEunXrhv3792Pw4MHw9vZGTEwMxo4di+vXryMyMhLAkz8/fvbZZ3jnnXcwdOhQAECDBg0APPkiz7Fjx9CnTx/UrVsX6enpWLZsGd577z2kpqaiVq1a5TrmYcOGYe3atRg0aBBGjRqFy5cvY8mSJTh16hSOHj0KY2NjTJw4Ef/9738xePBgpKSkwMLCAjExMVi5ciWmTZuGJk2aICsrC1OnTsXkyZMxdOhQtGnTBkD5/dmQ6FVV1mt92LBhqFOnDmbOnIlRo0ahefPmsLe3BwDs2bMHnTt3Rv369REeHo4HDx5g8eLF8PPzw8mTJ4utzPXq1QtvvvkmZs6cqbd48DIKCgoQGBiI1q1bY968edJ7y5YtW3D//n2MGDECtra2SExMxOLFi/H7779jy5Ytz+33yJEj2Lp1Kz7//HNYWFjg66+/RlBQEK5evQpbW9tXGjPR8zDO/zRjxgxMmjQJvXv3xmeffYY//vgDixcvRtu2bXHq1ClYWVnh0aNHCAwMRH5+PkaOHAmNRoPr169j586duHPnDiwtLZ+Zs9RIgmqMkJAQ8fQp3759uwAgpk+frtfuww8/FAqFQly6dEkqU6lUIjg4uFif9+/fL1YWHx8vAIhvv/1WKtu/f78AIPbv3//S4z18+LAAINavX6/XLjo6ulh5SkqKMDExEZ999pm4ffu2qFOnjmjWrJl4/Pix1ObEiRMCgFizZs0Lj4nodahqr/WieN2yZYteube3t7CzsxP/+9//pLLTp08LAwMDMWDAAKlsypQpAoD4+OOPX+j5njZ37lwBQFy+fFkqCw4OFgDE+PHji7Uv6T0oIiJCKBQKceXKlWJjehoAYWJiovded/r0aQFALF68uMxjJ3oWxvmf/hrn6enpwtDQUMyYMUOvXUpKijAyMpLKT506VeKY/6q0nKUm4qU7NdhPP/0EQ0NDjBo1Sq/873//O4QQ2L1793P7MDMzk/79+PFj/O9//4ObmxusrKxw8uTJch3vli1bYGlpiY4dO+LmzZvS5uvrC3Nzc+zfv19q+/bbb+Orr77CqlWrEBgYiJs3byIqKorX6FK1UBVf6xkZGUhOTsbAgQNhY2MjlTdu3BgdO3bETz/9VGyf4cOHl+sYRowYUazs6fegvLw83Lx5E61atYIQAqdOnXpun/7+/nqrfY0bN4ZarcZvv/1WPoMmKgXj/E9bt26FTqdD79699Y6FRqPBm2++KR0LS0tLAEBMTAzu37//ys9bEzDrqcGuXLkCR0dHWFhY6JUX3QHgypUrz+3jwYMHiIiIwJo1a3D9+nW9P9sVXS9XXi5evIicnBzY2dmVWF/0xaIiY8eOxcaNG5GYmIiZM2fCy8urXMdD9LpUxdd60fuBu7t7sTpPT0/ExMQU+yKeq6truT2/kZER6tatW6z86tWrmDx5Mn788Ue965qBF3sPqlevXrEya2vrYn0RlTfG+Z8uXrwIIQTefPPNEuuNjY2l5xozZgwWLFiA9evXo02bNujWrRs++eQT6UMA6WOiT69k5MiRWLNmDcLCwqDVamFpaQmFQoE+ffpAp9OV63PpdDrY2dlh/fr1Jda/8cYbeo9/++03XLx4EQCQkpJSrmMhep3k8lp/erX9VSmVShgY6P8RurCwEB07dsStW7fwxRdfwMPDAyqVCtevX8fAgQNf6D3I0NCwxHLxit8pIHoexvmfdDodFAoFdu/eXWJMmpubS/+eP38+Bg4ciB07diA2NhajRo1CREQEjh8/XuJiQE3HRL8Gc3Z2xp49e3D37l29Vf3z589L9UVK+0XJ77//HsHBwZg/f75U9vDhQ9y5c6fcx9ugQQPs2bMHfn5+z31j0el0GDhwINRqNcLCwqR76vbs2VNqw1/JpKqqKr7Wi94P0tLSitWdP38etWvXLtPtLstDSkoKLly4gKioKAwYMEAqj4uLq9BxEL0MxvmfGjRoACEEXF1d8dZbbz23faNGjdCoUSNMnDgRx44dg5+fH5YvX47p06cD4P/vT+M1+jXY+++/j8LCQixZskSvPDIyEgqFAp07d5bKVCpVicm7oaFhsZWvxYsXo7CwsNzH27t3bxQWFmLatGnF6goKCvTGt2DBAhw7dgwrVqzAtGnT0KpVK4wYMQI3b96U2hS9Wb2ODyVEr6IqvtYdHBzg7e2NqKgovX5++eUXxMbG4v3333/pvl9W0crf0+9BQggsWrSowsdCVFaM8z/17NkThoaG+Oqrr4rlFEII/O9//wMA5ObmoqCgQK++UaNGMDAwQH5+vlRWWs5SE3FFvwb74IMP0K5dO/zzn/9Eeno6mjRpgtjYWOzYsQNhYWF6X1Dz9fXFnj17sGDBAjg6OsLV1RUtWrRA165dsW7dOlhaWsLLywvx8fHYs2fPa7kt3bvvvothw4YhIiICycnJCAgIgLGxMS5evIgtW7Zg0aJF+PDDD3Hu3DlMmjQJAwcOxAcffAAAWLt2Lby9vfH5559j8+bNAJ6sIFhZWWH58uWwsLCASqVCixYtyvW6YqKXUVVf63PnzkXnzp2h1WoxePBg6bZ7lpaWCA8PL+/D8FweHh5o0KAB/vGPf+D69etQq9X44YcfeH09VQuM8z81aNAA06dPx4QJE5Ceno4ePXrAwsICly9fxrZt2zB06FD84x//wL59+xAaGopevXrhrbfeQkFBAdatWwdDQ0MEBQVJ/ZWWs9RIlXS3H6oEf721lxBC3L17V4wePVo4OjoKY2Nj8eabb4q5c+cKnU6n1+78+fOibdu2wszMTACQblt1+/ZtMWjQIFG7dm1hbm4uAgMDxfnz54Wzs7Pera3K4/aaRVasWCF8fX2FmZmZsLCwEI0aNRLjxo0TN27cEAUFBaJ58+aibt264s6dO3r7LVq0SAAQmzZtksp27NghvLy8hJGREW+1SZWmqr3WS7vtnhBC7NmzR/j5+QkzMzOhVqvFBx98IFJTU/XaFN12748//ijjkSj99poqlarE9qmpqcLf31+Ym5uL2rVriyFDhki3yHx6jqXdXjMkJKRYn399/yIqD4zzP5UU50II8cMPP4jWrVsLlUolVCqV8PDwECEhISItLU0IIcRvv/0mPv30U9GgQQNhamoqbGxsRLt27cSePXv0+iktZ6mJFELwG0dERERERHLDa/SJiIiIiGSIiT4RERERkQwx0SciIiIikiEm+kREREREMsREn4iIiIhIhpjoU4139+5dhIWFwdnZGWZmZmjVqhVOnDgh1QshMHnyZDg4OMDMzAz+/v7Sz5A/bdeuXWjRogXMzMxgbW2NHj166NVfvXoVXbp0Qa1atWBnZ4exY8cW++EPIiIiovIi2x/M0ul0uHHjBiwsLPhTyPRMAwcOxLlz57B8+XJoNBps3rwZ/v7+SEhIgKOjIyIjI/H1119j2bJlcHZ2xowZM9CxY0ckJibC1NQUALBjxw6MGjUKkydPxrJly1BQUIDU1FTk5uYCAAoLC9G5c2fY2dkhNjYWWVlZGDZsGHQ6HaZMmVKZ05cIIXD37l04OjrCwKBqrgEwronKhnFNJD9liWvZ3kf/999/h5OTU2UPg6jauXbtGurWrVvZwygR45ro5TCuieTnReJativ6FhYWAJ4cBLVaXcmjoarq7t27qFu3Lnbs2IH33ntPKg8MDISRkRGWLFkCb29vHD58GI0bN5bq33//fTRq1AizZ89GUlIS2rdvj6VLl2L58uXIzs5Go0aNMG3aNHh5eQEAZsyYgd27d+PIkSNSH+np6WjSpAkOHTqEJk2aVNicS5ObmwsnJycpdqoixjVR2TCuieSnLHEt20S/6M9/arWabxxUKrVaDa1WiwULFqBZs2awt7fHd999h8TERLi5uSEvLw8A0KBBA73XkaOjI27dugW1Wo2srCwAwOzZs7FgwQK4uLhg/vz56Nq1Ky5cuAAbGxvcvn0bDg4Oen24ubkBAO7du1elXqNV+U/njGuil8O4JpKfF4nrqnnBHlEFWrduHYQQqFOnDpRKJb7++mt8/PHHL3w9q06nAwD885//RFBQEHx9fbFmzRooFAps2bLldQ6diIiIqFRM9KnGa9CgAQ4ePIh79+7h2rVrSExMxOPHj1G/fn1oNBoAkFbti2RlZUl1Dg4OACBdpgMASqUS9evXx9WrVwEAGo2mxD6K6oiIiIjKGxN9ov+jUqng4OCA27dvIyYmBt27d4erqys0Gg327t0rtcvNzUVCQgK0Wi0AwNfXF0qlEmlpaVKbx48fIz09Hc7OzgAArVaLlJQUZGdnS23i4uKgVqv1PiAQERERlRfZXqNP9KJiYmIghIC7uzsuXbqEsWPHwsPDA4MGDYJCoUBYWBimT5+ON998E66urpg0aRIcHR2l++Sr1WoMHz4cU6ZMgZOTE5ydnTF37lwAQK9evQAAAQEB8PLyQv/+/TFnzhxkZmZi4sSJCAkJgVKprKypExERkYwx0acaLycnBxMmTMDvv/8OGxsbBAUFYcaMGTA2NgYAjBs3Dnl5eRg6dCju3LmD1q1bIzo6WrqHPgDMnTsXRkZG6N+/Px48eIAWLVpg3759sLa2BgAYGhpi586dGDFiBLRaLVQqFYKDgzF16tRKmTMRERHJn2zvo5+bmwtLS0vk5OTwW/xEL6A6xEx1GCNRVVIdYqY6jJGoKilLzPAafSIiIiIiGWKiT0REREQkQ7xGn6o0l/G7KnsIspE+q0tlD4GIiIgqEFf0iYiInlJYWIhJkybB1dUVZmZmaNCgAaZNm4bSvtI2fPhwKBQKLFy4UK/85MmT6NixI6ysrGBra4uhQ4fi3r17Uv3//vc/dOrUCY6OjlAqlXByckJoaChyc3Nf5/SIqAZhok9ERPSU2bNnY9myZViyZAnOnTuH2bNnY86cOVi8eHGxttu2bcPx48fh6OioV37jxg34+/vDzc0NCQkJiI6OxtmzZzFw4ECpjYGBAbp3744ff/wRFy5cwNq1a7Fnzx4MHz78dU+RiGoIXrpDRET0lGPHjqF79+7o0uXJ5W4uLi747rvvkJiYqNfu+vXrGDlyJGJiYqS2RXbu3AljY2MsXboUBgZP1tSWL1+Oxo0b49KlS3Bzc4O1tTVGjBgh7ePs7IzPP/9c+h0OIqJXxRV9IiKip7Rq1Qp79+7FhQsXAACnT5/GkSNH0LlzZ6mNTqdD//79MXbsWDRs2LBYH/n5+TAxMZGSfAAwMzMDABw5cqTE571x4wa2bt2Kd999tzynQ0Q1GBN9IiKip4wfPx59+vSBh4cHjI2N4ePjg7CwMPTr109qM3v2bBgZGWHUqFEl9tG+fXtkZmZi7ty5ePToEW7fvo3x48cDADIyMvTafvzxx6hVqxbq1KkDtVqNVatWvb7JEVGNwkSfiIjoKZs3b8b69euxYcMGnDx5ElFRUZg3bx6ioqIAAElJSVi0aBHWrl0LhUJRYh8NGzZEVFQU5s+fj1q1akGj0cDV1RX29vZ6q/wAEBkZiZMnT2LHjh349ddfMWbMmNc+RyKqGXiNPhER0VPGjh0rreoDQKNGjXDlyhVEREQgODgYhw8fRnZ2NurVqyftU1hYiL///e9YuHAh0tPTAQB9+/ZF3759kZWVBZVKBYVCgQULFqB+/fp6z6fRaKDRaODh4QEbGxu0adMGkyZNgoODQ4XNmYjkiYk+ERHRU+7fv19s1d3Q0BA6nQ4A0L9/f/j7++vVBwYGon///hg0aFCx/uzt7QEA33zzDUxNTdGxY8dSn7voOfLz819pDkREABN9IiIiPR988AFmzJiBevXqoWHDhjh16hQWLFiATz/9FABga2sLW1tbvX2MjY2h0Wjg7u4ulS1ZsgStWrWCubk54uLiMHbsWMyaNQtWVlYAgJ9++glZWVlo3rw5zM3NcfbsWYwdOxZ+fn5wcXGpqOkSkYwx0SciInrK4sWLMWnSJHz++efIzs6Go6Mjhg0bhsmTJ5epn8TEREyZMgX37t2Dh4cH/v3vf6N///5SvZmZGVauXInRo0cjPz8fTk5O6Nmzp/SlXSKiV8VEn4iI6CkWFhZYuHBhsV+6fZai6/Kf9u233z5zn3bt2uHYsWNlHB0R0YvjXXeIiIiIiGSIiT4RERERkQzx0h0iIqoRXMbvquwhlLv0WV0qewhEVIVxRZ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpKhMiX6hYWFmDRpElxdXWFmZoYGDRpg2rRpEEJIbYQQmDx5MhwcHGBmZgZ/f39cvHhRr59bt26hX79+UKvVsLKywuDBg3Hv3j29NmfOnEGbNm1gamoKJycnzJkz5xWmSURERERUs5Qp0Z89ezaWLVuGJUuW4Ny5c5g9ezbmzJmDxYsXS23mzJmDr7/+GsuXL0dCQgJUKhUCAwPx8OFDqU2/fv1w9uxZxMXFYefOnTh06BCGDh0q1efm5iIgIADOzs5ISkrC3LlzER4ejhUrVpTDlImIiIiI5K9Mv4x77NgxdO/eHV26PPklPhcXF3z33XdITEwE8GQ1f+HChZg4cSK6d+8OAPj2229hb2+P7du3o0+fPjh37hyio6Nx4sQJNGvWDACwePFivP/++5g3bx4cHR2xfv16PHr0CN988w1MTEzQsGFDJCcnY8GCBXofCIiIiIiIqGRlWtFv1aoV9u7diwsXLgAATp8+jSNHjqBz584AgMuXLyMzMxP+/v7SPpaWlmjRogXi4+MBAPHx8bCyspKSfADw9/eHgYEBEhISpDZt27aFiYmJ1CYwMBBpaWm4fft2iWPLz89Hbm6u3kZE1Rvjmkh+GNdEFadMif748ePRp08feHh4wNjYGD4+PggLC0O/fv0AAJmZmQAAe3t7vf3s7e2luszMTNjZ2enVGxkZwcbGRq9NSX08/Rx/FRERAUtLS2lzcnIqy9SIqApiXBPJD+OaqOKUKdHfvHkz1q9fjw0bNuDkyZOIiorCvHnzEBUV9brG98ImTJiAnJwcabt27VplD4mIXhHjmkh+GNdEFadM1+iPHTtWWtUHgEaNGuHKlSuIiIhAcHAwNBoNACArKwsODg7SfllZWfD29gYAaDQaZGdn6/VbUFCAW7duSftrNBpkZWXptSl6XNTmr5RKJZRKZVmmQ0RVHOOaSH4Y10QVp0wr+vfv34eBgf4uhoaG0Ol0AABXV1doNBrs3btXqs/NzUVCQgK0Wi0AQKvV4s6dO0hKSpLa7Nu3DzqdDi1atJDaHDp0CI8fP5baxMXFwd3dHdbW1mWcIhERERFRzVOmRP+DDz7AjBkzsGvXLqSnp2Pbtm1YsGAB/t//+38AAIVCgbCwMEyfPh0//vgjUlJSMGDAADg6OqJHjx4AAE9PT3Tq1AlDhgxBYmIijh49itDQUPTp0weOjo4AgL59+8LExASDBw/G2bNnsWnTJixatAhjxowp39kTEREREclUmS7dWbx4MSZNmoTPP/8c2dnZcHR0xLBhwzB58mSpzbhx45CXl4ehQ4fizp07aN26NaKjo2Fqaiq1Wb9+PUJDQ9GhQwcYGBggKCgIX3/9tVRvaWmJ2NhYhISEwNfXF7Vr18bkyZN5a00iIiIiohekEE//rK2M5ObmwtLSEjk5OVCr1ZU9HHpJLuN3VfYQZCN9Vpdn1leHmKkOY6SqS47vJ4xropqnLDFTpkt3iIiIiIioemCiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIAwPXr1/HJJ5/A1tYWZmZmaNSoEX7++WepPjw8HB4eHlCpVLC2toa/vz8SEhL0+nBxcYFCodDbZs2aJdWnp6cXq1coFDh+/HiFzZOIqKYwquwBEBFR5bt9+zb8/PzQrl077N69G2+88QYuXrwIa2trqc1bb72FJUuWoH79+njw4AEiIyMREBCAS5cu4Y033pDaTZ06FUOGDJEeW1hYFHu+PXv2oGHDhtJjW1vb1zQzIqKai4k+ERFh9uzZcHJywpo1a6QyV1dXvTZ9+/bVe7xgwQKsXr0aZ86cQYcOHaRyCwsLaDSaZz6fra3tc9sQEdGr4aU7RESEH3/8Ec2aNUOvXr1gZ2cHHx8frFy5stT2jx49wooVK2BpaYkmTZro1c2aNQu2trbw8fHB3LlzUVBQUGz/bt26wc7ODq1bt8aPP/5Y7vMhIiKu6BMREYDffvsNy5Ytw5gxY/Dll1/ixIkTGDVqFExMTBAcHCy127lzJ/r06YP79+/DwcEBcXFxqF27tlQ/atQoNG3aFDY2Njh27BgmTJiAjIwMLFiwAABgbm6O+fPnw8/PDwYGBvjhhx/Qo0cPbN++Hd26davweRMRyRkTfSIigk6nQ7NmzTBz5kwAgI+PD3755RcsX75cL9Fv164dkpOTcfPmTaxcuRK9e/dGQkIC7OzsAABjxoyR2jZu3BgmJiYYNmwYIiIioFQqUbt2bb02zZs3x40bNzB37lwm+kRE5YyX7hARERwcHODl5aVX5unpiatXr+qVqVQquLm5oWXLlli9ejWMjIywevXqUvtt0aIFCgoKkJ6e/sw2ly5deqXxExFRcUz0iYgIfn5+SEtL0yu7cOECnJ2dn7mfTqdDfn5+qfXJyckwMDCQVvxLa+Pg4FC2ARMR0XPx0h0iIsLo0aPRqlUrzJw5E71790ZiYiJWrFiBFStWAADy8vIwY8YMdOvWDQ4ODrh58yaWLl2K69evo1evXgCA+Ph4JCQkoF27drCwsEB8fDxGjx6NTz75RLpNZ1RUFExMTODj4wMA2Lp1K7755husWrWqciZORCRjTPSJiAjNmzfHtm3bMGHCBEydOhWurq5YuHAh+vXrBwAwNDTE+fPnERUVhZs3b8LW1hbNmzfH4cOHpfvhK5VKbNy4EeHh4cjPz4erqytGjx6td00+AEybNg1XrlyBkZERPDw8sGnTJnz44YcVPmciIrljok9ERACArl27omvXriXWmZqaYuvWrc/cv2nTps/9hdvg4GC9L/cSEdHrw2v0iYiIiIhkiIk+EREREZEM8dIdIiIZcxm/q7KHUO7SZ3Wp7CEQEVULXNEnIiIiIpIhJvpERERERDLERJ+IiIiISIbKnOhfv34dn3zyCWxtbWFmZoZGjRrh559/luqFEJg8eTIcHBxgZmYGf39/XLx4Ua+PW7duoV+/flCr1bCyssLgwYNx7949vTZnzpxBmzZtYGpqCicnJ8yZM+clp0hEREREVPOUKdG/ffs2/Pz8YGxsjN27dyM1NRXz58+XfvEQAObMmYOvv/4ay5cvR0JCAlQqFQIDA/Hw4UOpTb9+/XD27FnExcVh586dOHToEIYOHSrV5+bmIiAgAM7OzkhKSsLcuXMRHh4u/UIjERERERE9W5nuujN79mw4OTlhzZo1Upmrq6v0byEEFi5ciIkTJ6J79+4AgG+//Rb29vbYvn07+vTpg3PnziE6OhonTpxAs2bNAACLFy/G+++/j3nz5sHR0RHr16/Ho0eP8M0338DExAQNGzZEcnIyFixYoPeBgIiIiIiISlamFf0ff/wRzZo1Q69evWBnZwcfHx+sXLlSqr98+TIyMzPh7+8vlVlaWqJFixaIj48HAMTHx8PKykpK8gHA398fBgYGSEhIkNq0bdsWJiYmUpvAwECkpaXh9u3bLzdTIiIiIqIapEyJ/m+//YZly5bhzTffRExMDEaMGIFRo0YhKioKAJCZmQkAsLe319vP3t5eqsvMzISdnZ1evZGREWxsbPTalNTH08/xV/n5+cjNzdXbiKh6Y1wTyQ/jmqjilCnR1+l0aNq0KWbOnAkfHx8MHToUQ4YMwfLly1/X+F5YREQELC0tpc3Jyamyh0REr4hxTSQ/jGuiilOmRN/BwQFeXl56ZZ6enrh69SoAQKPRAACysrL02mRlZUl1Go0G2dnZevUFBQW4deuWXpuS+nj6Of5qwoQJyMnJkbZr166VZWpEVAUxronkh3FNVHHKlOj7+fkhLS1Nr+zChQtwdnYG8OSLuRqNBnv37pXqc3NzkZCQAK1WCwDQarW4c+cOkpKSpDb79u2DTqdDixYtpDaHDh3C48ePpTZxcXFwd3fXu8PP05RKJdRqtd5GRNUb45pIfhjXRBWnTIn+6NGjcfz4ccycOROXLl3Chg0bsGLFCoSEhAAAFAoFwsLCMH36dPz4449ISUnBgAED4OjoiB49egB48heATp06YciQIUhMTMTRo0cRGhqKPn36wNHREQDQt29fmJiYYPDgwTh79iw2bdqERYsWYcyYMeU7eyIiIiIimSrT7TWbN2+Obdu2YcKECZg6dSpcXV2xcOFC9OvXT2ozbtw45OXlYejQobhz5w5at26N6OhomJqaSm3Wr1+P0NBQdOjQAQYGBggKCsLXX38t1VtaWiI2NhYhISHw9fVF7dq1MXnyZN5ak4iIiIjoBZUp0QeArl27omvXrqXWKxQKTJ06FVOnTi21jY2NDTZs2PDM52ncuDEOHz5c1uERERERERHKeOkOERERERFVD0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIZeKdGfNWsWFAoFwsLCpLKHDx8iJCQEtra2MDc3R1BQELKysvT2u3r1Krp06YJatWrBzs4OY8eORUFBgV6bAwcOoGnTplAqlXBzc8PatWtfZahEREREVEMtW7YMjRs3hlqthlqthlarxe7du/XaxMfHo3379lCpVFCr1Wjbti0ePHhQrK/8/Hx4e3tDoVAgOTlZr+7MmTNo06YNTE1N4eTkhDlz5rzOaT3XSyf6J06cwL///W80btxYr3z06NH473//iy1btuDgwYO4ceMGevbsKdUXFhaiS5cuePToEY4dO4aoqCisXbsWkydPltpcvnwZXbp0Qbt27ZCcnIywsDB89tlniImJednhEhEREVENVbduXcyaNQtJSUn4+eef0b59e3Tv3h1nz54F8CTJ79SpEwICApCYmIgTJ04gNDQUBgbFU+Vx48bB0dGxWHlubi4CAgLg7OyMpKQkzJ07F+Hh4VixYsVrn19pjF5mp3v37qFfv35YuXIlpk+fLpXn5ORg9erV2LBhA9q3bw8AWLNmDTw9PXH8+HG0bNkSsbGxSE1NxZ49e2Bvbw9vb29MmzYNX3zxBcLDw2FiYoLly5fD1dUV8+fPBwB4enriyJEjiIyMRGBgYDlMm4iIiIhqig8++EDv8YwZM7Bs2TIcP34cDRs2xOjRozFq1CiMHz9eauPu7l6sn927dyM2NhY//PBDsb8IrF+/Ho8ePcI333wDExMTNGzYEMnJyViwYAGGDh36eib2HC+1oh8SEoIuXbrA399frzwpKQmPHz/WK/fw8EC9evUQHx8P4MknpkaNGsHe3l5qExgYiNzcXL1PVX/tOzAwUOqjJPn5+cjNzdXbiKh6Y1wTyQ/jmipbYWEhNm7ciLy8PGi1WmRnZyMhIQF2dnZo1aoV7O3t8e677+LIkSN6+2VlZWHIkCFYt24datWqVazf+Ph4tG3bFiYmJlJZYGAg0tLScPv27dc+r5KUOdHfuHEjTp48iYiIiGJ1mZmZMDExgZWVlV65vb09MjMzpTZPJ/lF9UV1z2qTm5tb4rVSABAREQFLS0tpc3JyKuvUiKiKYVwTyQ/jmipLSkoKzM3NoVQqMXz4cGzbtg1eXl747bffAADh4eEYMmQIoqOj0bRpU3To0AEXL14EAAghMHDgQAwfPhzNmjUrsf8XyXErWpkS/WvXruFvf/sb1q9fD1NT09c1ppcyYcIE5OTkSNu1a9cqe0hE9IoY10Tyw7imyuLu7o7k5GQkJCRgxIgRCA4ORmpqKnQ6HQBg2LBhGDRoEHx8fBAZGQl3d3d88803AIDFixfj7t27mDBhQmVOoczKdI1+UlISsrOz0bRpU6mssLAQhw4dwpIlSxATE4NHjx7hzp07eqv6WVlZ0Gg0AACNRoPExES9fovuyvN0m7/eqScrKwtqtRpmZmYljk2pVEKpVJZlOkRUxTGuieSHcU2VxcTEBG5ubgAAX19fnDhxAosWLZKuy/fy8tJr7+npiatXrwIA9u3bh/j4+GKv3WbNmqFfv36IiooqNX8F/sxxK1qZVvQ7dOiAlJQUJCcnS1vRBIv+bWxsjL1790r7pKWl4erVq9BqtQAArVaLlJQUZGdnS23i4uKgVqulA6zVavX6KGpT1AcRERER0avQ6XTIz8+Hi4sLHB0dkZaWpld/4cIFODs7AwC+/vprnD59Wsp/f/rpJwDApk2bMGPGDABP8tdDhw7h8ePHUh9xcXFwd3eHtbV1Bc1KX5lW9C0sLPD222/rlalUKtja2krlgwcPxpgxY2BjYwO1Wo2RI0dCq9WiZcuWAICAgAB4eXmhf//+mDNnDjIzMzFx4kSEhIRIn5KGDx+OJUuWYNy4cfj000+xb98+bN68Gbt27SqPORMRERFRDTJhwgR07twZ9erVw927d7FhwwYcOHAAMTExUCgUGDt2LKZMmYImTZrA29sbUVFROH/+PL7//nsAQL169fT6Mzc3BwA0aNAAdevWBQD07dsXX331FQYPHowvvvgCv/zyCxYtWoTIyMiKnexTXur2ms8SGRkJAwMDBAUFIT8/H4GBgfjXv/4l1RsaGmLnzp0YMWIEtFotVCoVgoODMXXqVKmNq6srdu3ahdGjR2PRokWoW7cuVq1axVtrEhEREVGZZWdnY8CAAcjIyIClpSUaN26MmJgYdOzYEQAQFhaGhw8fYvTo0bh16xaaNGmCuLg4NGjQ4IWfw9LSErGxsQgJCYGvry9q166NyZMnV9qtNQFAIYQQlfbsr1Fubi4sLS2Rk5MDtVpd2cOhl+Qynn/FKS/ps7o8s746xEx1GGNVI8cYet5ruTQ18VhUh5ipDmMkqkrKEjMv/cu4RERERERUdTHRJyIiIiKSoXK/Rp+IiIiIqCLUxEvyyoIr+hVg2bJlaNy4MdRqNdRqNbRaLXbv3i3VP3z4ECEhIbC1tYW5uTmCgoL07sO6du1aKBSKErei25QeOXIEfn5+sLW1hZmZGTw8PCr1W95EREREVLm4ol8B6tati1mzZuHNN9+EEAJRUVHo3r07Tp06hYYNG2L06NHYtWsXtmzZAktLS4SGhqJnz544evQoAOCjjz5Cp06d9PocOHAgHj58CDs7OwBPbnMaGhqKxo0bQ6VS4ciRIxg2bBhUKlWlftubiIiIiCoHE/0K8MEHH+g9njFjBpYtW4bjx4+jbt26WL16NTZs2ID27dsDANasWQNPT08cP34cLVu2hJmZmd4vAv/xxx/Yt28fVq9eLZX5+PjAx8dHeuzi4oKtW7fi8OHDTPSJiIiIaiBeulPBCgsLsXHjRuTl5UGr1SIpKQmPHz+Gv7+/1MbDwwP16tVDfHx8iX18++23qFWrFj788MNSn+fUqVM4duwY3n333XKfAxERERFVfVzRryApKSnQarV4+PAhzM3NsW3bNnh5eSE5ORkmJiawsrLSa29vb4/MzMwS+1q9ejX69u2rt8pfpG7duvjjjz9QUFCA8PBwfPbZZ69jOkRERERUxTHRryDu7u5ITk5GTk4Ovv/+ewQHB+PgwYNl7ic+Ph7nzp3DunXrSqw/fPgw7t27h+PHj2P8+PFwc3PDxx9//KrDJyIiIqJqhol+BTExMYGbmxsAwNfXFydOnMCiRYvw0Ucf4dGjR7hz547eqn5WVhY0Gk2xflatWgVvb2/4+vqW+Dyurq4AgEaNGiErKwvh4eFM9ImIiIhqIF6jX0l0Oh3y8/Ph6+sLY2Nj7N27V6pLS0vD1atXodVq9fa5d+8eNm/ejMGDB5fpOYiIiIio5uGKfgWYMGECOnfujHr16uHu3bvYsGEDDhw4gJiYGFhaWmLw4MEYM2YMbGxsoFarMXLkSGi1WrRs2VKvn02bNqGgoACffPJJsedYunQp6tWrBw8PDwDAoUOHMG/ePIwaNapC5khEREREVQtX9CtAdnY2BgwYAHd3d3To0AEnTpxATEwMOnbsCACIjIxE165dERQUhLZt20Kj0WDr1q3F+lm9ejV69uxZ7Iu7wJPV+wkTJsDb2xvNmjXD0qVLMXv2bEydOvV1T4+IiIiqoIiICDRv3hwWFhaws7NDjx49kJaWJtWnp6eX+oOcW7ZskdpdvXoVXbp0Qa1atWBnZ4exY8eioKBAqj9w4ECJfZR2UxGqOFzRrwBP3+++JKampli6dCmWLl36zHbHjh0rtW7kyJEYOXLkS42PiIiI5OfgwYMICQlB8+bNUVBQgC+//BIBAQFITU2FSqWCk5MTMjIy9PZZsWIF5s6di86dOwN4clvwLl26QKPR4NixY8jIyMCAAQNgbGyMmTNn6u2blpYGtVotPS76UU+qPEz0iYiIiGQoOjpa7/HatWthZ2eHpKQktG3bFoaGhsVu/LFt2zb07t0b5ubmAIDY2FikpqZiz549sLe3h7e3N6ZNm4YvvvgC4eHhMDExkfa1s7Mr8aoDqjy8dIeIiIioBsjJyQEA2NjYlFiflJSE5ORkvZt+xMfHo1GjRrC3t5fKAgMDkZubi7Nnz+rt7+3tDQcHB3Ts2BFHjx59DTOgsqrxK/ou43dV9hBkI31Wl8oeAhEREZVAp9MhLCwMfn5+ePvtt0tss3r1anh6eqJVq1ZSWWZmpl6SD0B6XHQNvoODA5YvX45mzZohPz8fq1atwnvvvYeEhAQ0bdr0Nc2IXkSNT/SJiIiI5C4kJAS//PILjhw5UmL9gwcPsGHDBkyaNKnMfbu7u8Pd3V163KpVK/z666+IjIws9Qc+qWLw0h0iIiIiGQsNDcXOnTuxf/9+1K1bt8Q233//Pe7fv48BAwbolWs0GmRlZemVFT0u6Yc9i7zzzju4dOnSK46cXhUTfSIiIiIZEkIgNDQU27Ztw759++Dq6lpq29WrV6Nbt25444039Mq1Wi1SUlKQnZ0tlcXFxUGtVsPLy6vU/pKTk+Hg4PDqk6BXwkt3iIiIiGQoJCQEGzZswI4dO2BhYSFdU29paQkzMzOp3aVLl3Do0CH89NNPxfoICAiAl5cX+vfvjzlz5iAzMxMTJ05ESEgIlEolAGDhwoVwdXVFw4YN8fDhQ6xatQr79u1DbGxsxUyUSsVEn4iIiEiGli1bBgB477339MrXrFmDgQMHSo+/+eYb1K1bFwEBAcX6MDQ0xM6dOzFixAhotVqoVCoEBwfr/SDno0eP8Pe//x3Xr19HrVq10LhxY+zZswft2rV7LfOiF8dEn4iIiEiGhBAv1G7mzJnFfvzqac7OziWu9hcZN24cxo0bV+bx0evHa/SJiIiIiGSIiT4RERERkQzx0h0iqrEiIiKwdetWnD9/HmZmZmjVqhVmz56tdz/oFStWYMOGDTh58iTu3r2L27dvF/uJ927duiE5ORnZ2dmwtraGv78/Zs+eDUdHR6lNTEwMpkyZgrNnz8LU1BRt27bF/Pnz4eLiUkGzJSK5kOOPffJHN18PrugTUY118OBBhISE4Pjx44iLi8Pjx48REBCAvLw8qc39+/fRqVMnfPnll6X2065dO2zevBlpaWn44Ycf8Ouvv+LDDz+U6i9fvozu3bujffv2SE5ORkxMDG7evImePXu+1vkREVHNxhV9IqqxoqOj9R6vXbsWdnZ2SEpKQtu2bQEAYWFhAIADBw6U2s/o0aOlfzs7O2P8+PHo0aMHHj9+DGNjYyQlJaGwsBDTp0+HgcGT9ZV//OMf6N69u9SGiIiovHFFn4jo/+Tk5AAAbGxsXrqPW7duYf369WjVqpWUwPv6+sLAwABr1qxBYWEhcnJysG7dOvj7+zPJJyKi14aJPhERAJ1Oh7CwMPj5+eHtt98u8/5ffPEFVCoVbG1tcfXqVezYsUOqc3V1RWxsLL788ksolUpYWVnh999/x+bNm8tzCkRERHqY6BMR4ckvSP7yyy/YuHHjS+0/duxYnDp1CrGxsTA0NMSAAQOke1hnZmZiyJAhCA4OxokTJ3Dw4EGYmJjgww8/fOH7XBMREZUVr9EnohovNDQUO3fuxKFDh1C3bt2X6qN27dqoXbs23nrrLXh6esLJyQnHjx+HVqvF0qVLYWlpiTlz5kjt//Of/8DJyQkJCQlo2bJleU2FiIhIwkSfiGosIQRGjhyJbdu24cCBA3B1dS2XfnU6HQAgPz8fwJM79xR9CbeIoaGhXlsiIqLyxkSfiGqskJAQbNiwATt27ICFhQUyMzMBAJaWljAzMwPw5LKbzMxMXLp0CQCQkpICCwsL1KtXDzY2NkhISMCJEyfQunVrWFtb49dff8WkSZPQoEEDaLVaAECXLl0QGRmJqVOn4uOPP8bdu3fx5ZdfwtnZGT4+PpUzeSIikj1eo09ENdayZcuQk5OD9957Dw4ODtK2adMmqc3y5cvh4+ODIUOGAADatm0LHx8f/PjjjwCAWrVqYevWrejQoQPc3d0xePBgNG7cGAcPHoRSqQQAtG/fHhs2bMD27dvh4+ODTp06QalUIjo6WvpAQUREVN64ok9ENdaLfBE2PDwc4eHhpdY3atQI+/bte24/ffr0QZ8+fcoyPCIiolfCFX0iIiIiIhkqU6IfERGB5s2bw8LCAnZ2dujRowfS0tL02jx8+BAhISGwtbWFubk5goKCkJWVpdfm6tWr6NKlC2rVqgU7OzuMHTsWBQUFem0OHDiApk2bQqlUws3NDWvXrn25GRIRERER1UBlunTn4MGDCAkJQfPmzVFQUIAvv/wSAQEBSE1NhUqlAvDkp+B37dqFLVu2wNLSEqGhoejZsyeOHj0KACgsLESXLl2g0Whw7NgxZGRkYMCAATA2NsbMmTMBAJcvX0aXLl0wfPhwrF+/Hnv37sVnn30GBwcHBAYGlvMhICK5cRm/q7KH8Fqkz+pS2UMgIqJqpEyJfnR0tN7jtWvXws7ODklJSWjbti1ycnKwevVqbNiwAe3btwcArFmzBp6enjh+/DhatmyJ2NhYpKamYs+ePbC3t4e3tzemTZuGL774AuHh4TAxMcHy5cvh6uqK+fPnAwA8PT1x5MgRREZGMtEnIiIiInoBr3SNfk5ODgDAxsYGAJCUlITHjx/D399fauPh4YF69eohPj4eABAfH49GjRrB3t5eahMYGIjc3FycPXtWavN0H0VtivogIiIiIqJne+m77uh0OoSFhcHPzw9vv/02gCf3mzYxMYGVlZVeW3t7e+n+1JmZmXpJflF9Ud2z2uTm5uLBgwcl3o4uPz9f+nEaAMjNzX3ZqRFRFcG4JpIfxjVRxXnpFf2QkBD88ssv2LhxY3mO56VFRETA0tJS2pycnCp7SET0ihjXRPLDuCaqOC+V6IeGhmLnzp3Yv38/6tatK5VrNBo8evQId+7c0WuflZUFjUYjtfnrXXiKHj+vjVqtLvXHZSZMmICcnBxpu3bt2stMjYiqEMY1kfwwrokqTpkSfSEEQkNDsW3bNuzbtw+urq569b6+vjA2NsbevXulsrS0NFy9elX6KXitVouUlBRkZ2dLbeLi4qBWq+Hl5SW1ebqPojZFfZREqVRCrVbrbURUvTGuieSHcU1Uccp0jX5ISAg2bNiAHTt2wMLCQrqm3tLSEmZmZrC0tMTgwYMxZswY2NjYQK1WY+TIkdBqtWjZsiUAICAgAF5eXujfvz/mzJmDzMxMTJw4ESEhIdLPxQ8fPhxLlizBuHHj8Omnn2Lfvn3YvHkzdu2S5y3ziIiIiIjKW5lW9JctW4acnBy89957cHBwkLZNmzZJbSIjI9G1a1cEBQWhbdu20Gg02Lp1q1RvaGiInTt3wtDQEFqtFp988gkGDBiAqVOnSm1cXV2xa9cuxMXFoUmTJpg/fz5WrVrFW2sSEREREb2gMq3oCyGe28bU1BRLly7F0qVLS23j7OyMn3766Zn9vPfeezh16lRZhkdERERERP/nle6jT0REREREVRMTfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIhk7+7duwgLC4OzszPMzMzQqlUrnDhxQqoPDw+Hh4cHVCoVrK2t4e/vj4SEBL0+XFxcoFAo9LZZs2ZV9FSIXhgTfSIiIpK9zz77DHFxcVi3bh1SUlIQEBAAf39/XL9+HQDw1ltvYcmSJUhJScGRI0fg4uKCgIAA/PHHH3r9TJ06FRkZGdI2cuTIypgO0Qthok9ERESy9uDBA/zwww+YM2cO2rZtCzc3N4SHh8PNzQ3Lli0DAPTt2xf+/v6oX78+GjZsiAULFiA3NxdnzpzR68vCwgIajUbaVCpVZUyJ6IUw0SciIiJZKygoQGFhIUxNTfXKzczMcOTIkWLtHz16hBUrVsDS0hJNmjTRq5s1axZsbW3h4+ODuXPnoqCg4LWOnehVGFX2AIiIiIheJwsLC2i1WkybNg2enp6wt7fHd999h/j4eLi5uUntdu7ciT59+uD+/ftwcHBAXFwcateuLdWPGjUKTZs2hY2NDY4dO4YJEyYgIyMDCxYsqIxpET0XE30iIiKSvXXr1uHTTz9FnTp1YGhoiKZNm+Ljjz9GUlKS1KZdu3ZITk7GzZs3sXLlSvTu3RsJCQmws7MDAIwZM0Zq27hxY5iYmGDYsGGIiIiAUqms8DkRPQ8v3SEiIiLZa9CgAQ4ePIh79+7h2rVrSExMxOPHj1G/fn2pjUqlgpubG1q2bInVq1fDyMgIq1evLrXPFi1aoKCgAOnp6RUwA6KyY6JPRERENYZKpYKDgwNu376NmJgYdO/evdS2Op0O+fn5pdYnJyfDwMBAWvEnqmp46Q4RERHJXkxMDIQQcHd3x6VLlzB27Fh4eHhg0KBByMvLw4wZM9CtWzc4ODjg5s2bWLp0Ka5fv45evXoBAOLj45GQkIB27drBwsIC8fHxGD16ND755BNYW1tX8uyISsZEn4iIiGQvJycHEyZMwO+//w4bGxsEBQVhxowZMDY2RmFhIc6fP4+oqCjcvHkTtra2aN68OQ4fPoyGDRsCAJRKJTZu3Ijw8HDk5+fD1dUVo0eP1rtun6iqYaJPREREste7d2/07t27xDpTU1Ns3br1mfs3bdoUx48ffx1DI3pteI0+EREREZEMMdEnIiIiIpIhXrpDRERE1YLL+F2VPYRylz6rS2UPgWSMK/pERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDLERJ+IiIiISIaY6BMRERERyRATfSIiIiIiGWKiT0REREQkQ0z0iYiIiIhkiIk+EREREZEMMdEnIiIiIpIhJvpERERERDJUpRP9pUuXwsXFBaampmjRogUSExMre0hERERERNVClU30N23ahDFjxmDKlCk4efIkmjRpgsDAQGRnZ1f20IiIiIiIqrwqm+gvWLAAQ4YMwaBBg+Dl5YXly5ejVq1a+Oabbyp7aEREREREVZ5RZQ+gJI8ePUJSUhImTJgglRkYGMDf3x/x8fEl7pOfn4/8/HzpcU5ODgAgNzf3mc+ly79fDiMm4PnH+mXw/JSf552fonohREUM54UwrvW9TIzJ8Vi87HtNTTwWcoproGaew5LwOPypJh6LMsW1qIKuX78uAIhjx47plY8dO1a88847Je4zZcoUAYAbN26vuF27dq0iwvyFMK65cSufjXHNjZv8theJa4UQVehj/v+5ceMG6tSpg2PHjkGr1Url48aNw8GDB5GQkFBsn7+uEOh0Oty6dQu2trZQKBQVMu7XJTc3F05OTrh27RrUanVlD4eeIqdzI4TA3bt34ejoCAODqnFVX1WPazmd/1fFY/FEVTsOjOuyq2rnsDLxWDxR1Y5DWeK6Sl66U7t2bRgaGiIrK0uvPCsrCxqNpsR9lEollEqlXpmVldXrGmKlUKvVVeIFRsXJ5dxYWlpW9hD0VJe4lsv5Lw88Fk9UpePAuH45VekcVjYeiyeq0nF40biuGh/v/8LExAS+vr7Yu3evVKbT6bB37169FX4iIiIiIipZlVzRB4AxY8YgODgYzZo1wzvvvIOFCxciLy8PgwYNquyhERERERFVeVU20f/oo4/wxx9/YPLkycjMzIS3tzeio6Nhb29f2UOrcEqlElOmTCn2p06qfDw3NRvP/594LJ7gcaj+eA7/xGPxRHU+DlXyy7hERERERPRqquQ1+kRERERE9GqY6BMRERERyRATfSIiIiIiGWKiXw2lp6dDoVAgOTm5sodCf8FzU7Px/P+pJh4LhUKB7du3l1pfkcfkeWOhF1cTX8ulqYnHorrHNRP9cjRw4EAoFApps7W1RadOnXDmzJnKHhrOnj2LoKAguLi4QKFQYOHChZU9pApVlc/NypUr0aZNG1hbW8Pa2hr+/v5ITEys7GHJSlU+/xUdm1X5WJRXLGRmZmLkyJGoX78+lEolnJyc8MEHH+j9NktlcHJyQkZGBt5+++1KHYdcVOXXMuP6T4zrysVEv5x16tQJGRkZyMjIwN69e2FkZISuXbtW9rBw//591K9fH7NmzSr114XlrqqemwMHDuDjjz/G/v37ER8fDycnJwQEBOD69euVPTRZqarnvzJis6oei/KIhfT0dPj6+mLfvn2YO3cuUlJSEB0djXbt2iEkJOQ1jv75DA0NodFoYGRUZe9sXe1U1dcy4/pPjOtKJqjcBAcHi+7du+uVHT58WAAQ2dnZUtmZM2dEu3bthKmpqbCxsRFDhgwRd+/eleoLCwvFV199JerUqSNMTExEkyZNxO7du6X6y5cvCwDi1KlTQgghCgoKxKBBg4S7u7u4cuXKc8fp7OwsIiMjX2mu1U11OTdF+1hYWIioqKiXnzDpqS7nvyJis7oci6J9yhoLnTt3FnXq1BH37t0rVnf79m0hhBBXrlwR3bp1EyqVSlhYWIhevXqJzMxMqd2UKVNEkyZNxOrVq4WTk5NQqVRixIgRoqCgQMyePVvY29uLN954Q0yfPl2vfwDiX//6l+jUqZMwNTUVrq6uYsuWLaUek/379wsAYs+ePcLX11eYmZkJrVYrzp8/r9fv9u3bhY+Pj1AqlcLV1VWEh4eLx48fS/UXLlwQbdq0EUqlUnh6eorY2FgBQGzbtu2Fj1t1VF1ey4xrfYzrJyoqrrmi/xrdu3cP//nPf+Dm5gZbW1sAQF5eHgIDA2FtbY0TJ05gy5Yt2LNnD0JDQ6X9Fi1ahPnz52PevHk4c+YMAgMD0a1bN1y8eLHYc+Tn56NXr15ITk7G4cOHUa9evQqbX3VWlc/N/fv38fjxY9jY2JTPZKmYqnz+K1pVPhZljYVbt24hOjoaISEhUKlUxeqtrKyg0+nQvXt33Lp1CwcPHkRcXBx+++03fPTRR3ptf/31V+zevRvR0dH47rvvsHr1anTp0gW///47Dh48iNmzZ2PixIlISEjQ22/SpEkICgrC6dOn0a9fP/Tp0wfnzp175rj/+c9/Yv78+fj5559hZGSETz/9VKo7fPgwBgwYgL/97W9ITU3Fv//9b6xduxYzZswAAOh0OvTs2RMmJiZISEjA8uXL8cUXX7zQ8ZKbqvxarmhV+Vgwris4rsv0sYCeKTg4WBgaGgqVSiVUKpUAIBwcHERSUpLUZsWKFcLa2lrvU+muXbuEgYGB9MnT0dFRzJgxQ6/v5s2bi88//1wI8eenx8OHD4sOHTqI1q1bizt37rzwOGvqin51ODdCCDFixAhRv3598eDBg5edLv1FdTn/FbXyVx2OhRBlj4WEhAQBQGzdurXUNrGxscLQ0FBcvXpVKjt79qwAIBITE4UQT1b+atWqJXJzc6U2gYGBwsXFRRQWFkpl7u7uIiIiQnoMQAwfPlzv+Vq0aCFGjBghhHj2yl+RXbt2CQDSnDt06CBmzpyp1+e6deuEg4ODEEKImJgYYWRkJK5fvy7V7969u8as6FeH1zLjWh/jumLjmiv65axdu3ZITk5GcnIyEhMTERgYiM6dO+PKlSsAgHPnzqFJkyZ6n0r9/Pyg0+mQlpaG3Nxc3LhxA35+fnr9+vn5Ffv0+PHHHyMvLw+xsbGwtLR8/ZOr5qrDuZk1axY2btyIbdu2wdTU9BVmS39VHc5/RakOx+JlYkG8wA+9nzt3Dk5OTnBycpLKvLy8YGVlpTd2FxcXWFhYSI/t7e3h5eUFAwMDvbLs7Gy9/rVabbHHz1v5a9y4sfRvBwcHAJD6PX36NKZOnQpzc3NpGzJkCDIyMnD//n1pPo6OjqWOQc6qw2u5olSHY8G4rvi4ZqJfzlQqFdzc3ODm5obmzZtj1apVyMvLw8qVK8v9ud5//32cOXMG8fHx5d63HFX1czNv3jzMmjULsbGxem8QVD6q+vmvSFX9WLxsLLz55ptQKBQ4f/78ywxVj7Gxsd5jhUJRYplOpyvX51IoFAAg9Xvv3j189dVXUgKXnJyMlJQUXLx4kYsBqPqv5YpU1Y8F47py4pqJ/mumUChgYGCABw8eAAA8PT1x+vRp5OXlSW2OHj0KAwMDuLu7Q61Ww9HREUePHtXr5+jRo/Dy8tIrGzFiBGbNmoVu3brh4MGDr38yMlOVzs2cOXMwbdo0REdHo1mzZuUwO3qeqnT+K1tVOhavEgs2NjYIDAzE0qVL9cZe5M6dO/D09MS1a9dw7do1qTw1NRV37twpNvaXcfz48WKPPT09X7q/pk2bIi0tTUrgnt4MDAyk+WRkZJQ6hpqkKr2WK1tVOhaMa30VGtdlutCHnik4OFh06tRJZGRkiIyMDJGamio+//xzoVAoxP79+4UQQuTl5QkHBwcRFBQkUlJSxL59+0T9+vVFcHCw1E9kZKRQq9Vi48aN4vz58+KLL74QxsbG4sKFC0KI4teDRUZGCnNzc3H48OFSx5afny9OnTolTp06JRwcHMQ//vEPcerUKXHx4sXXdTiqlKp8bmbNmiVMTEzE999/L40vIyND704I9Gqq8vmv6NisyseiPGLh119/FRqNRnh5eYnvv/9eXLhwQaSmpopFixYJDw8PodPphLe3t2jTpo1ISkoSCQkJwtfXV7z77rtSH0V35/jrcfvrXU3effdd8be//U16DEDUrl1brF69WqSlpYnJkycLAwMDcfbs2RKPSdG1vEV3DRFCiFOnTgkA4vLly0IIIaKjo4WRkZEIDw8Xv/zyi0hNTRXfffed+Oc//ymEeHKXFC8vL9GxY0eRnJwsDh06JHx9fWvMNfpV9bXMuP4T47py45qJfjkKDg4WAKTNwsJCNG/eXHz//fd67V7k9lbh4eGiTp06wtjY+Lm3txJCiPnz5wsLCwtx9OjREsdWtM9ft6eDQM6q8rlxdnYu8dxMmTKlXI9BTVaVz39Fx2ZVPhblFQs3btwQISEhwtnZWZiYmIg6deqIbt26SQnPi96G76/H7UUSgqVLl4qOHTsKpVIpXFxcxKZNm0o9Ji+SEAjxJClo1aqVMDMzE2q1WrzzzjtixYoVUn1aWppo3bq1MDExEW+99ZaIjo6uMYl+VX0tM67/xLiu3LhW/N8kiIiIiIhIRniNPhERERGRDDHRJyIiIiKSISb6REREREQyxESfiIiIiEiGmOgTEREREckQE30iIiIiIhliok9EREREJENM9ImIiIiIZIiJPhERERGRDDHRJyIiIiKSISb6REREREQyxESfiIiIiEiG/j844hR5rperfgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Preparing Vectors"
      ],
      "metadata": {
        "id": "sogmjbhvM4Wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorizes with term frequency-inverse document frequency (bag of words)"
      ],
      "metadata": {
        "id": "YWMHl1qAOcCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "# stopwors code\n",
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# from nltk.corpus import stopwords as st\n",
        "# vectorizer = CountVectorizer(stop_words = st.words('english'))\n",
        "\n",
        "# can include stopwords\n",
        "vectorizer = CountVectorizer(ngram_range = (1, 2))\n",
        "# train on only test\n",
        "x_vector_train = vectorizer.fit_transform(x_train)\n",
        "# fit test\n",
        "x_vector_test = vectorizer.transform(x_test)\n",
        "\n",
        "# show first entry of train\n",
        "print(x_train[0], \"\\n\", x_vector_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_zUT8z1NjTs",
        "outputId": "12e6612e-0a42-47f1-c664-5302d3e46885"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0          EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
            "0    You will rejoice to hear that no disaster has ...\n",
            "Name: Sentence, dtype: object \n",
            "   (0, 52626)\t2\n",
            "  (0, 51985)\t2\n",
            "  (0, 97832)\t2\n",
            "  (0, 29325)\t1\n",
            "  (0, 103797)\t1\n",
            "  (0, 48992)\t2\n",
            "  (0, 119978)\t1\n",
            "  (0, 35874)\t1\n",
            "  (0, 59606)\t1\n",
            "  (0, 106390)\t2\n",
            "  (0, 58390)\t1\n",
            "  (0, 8894)\t1\n",
            "  (0, 48321)\t1\n",
            "  (0, 42871)\t1\n",
            "  (0, 40798)\t1\n",
            "  (0, 40003)\t1\n",
            "  (0, 85100)\t1\n",
            "  (0, 45127)\t1\n",
            "  (0, 63294)\t3\n",
            "  (0, 120636)\t1\n",
            "  (0, 86969)\t1\n",
            "  (0, 83308)\t1\n",
            "  (0, 52826)\t2\n",
            "  (0, 52452)\t1\n",
            "  (0, 98854)\t1\n",
            "  (0, 29327)\t1\n",
            "  (0, 103828)\t1\n",
            "  (0, 49725)\t1\n",
            "  (0, 101773)\t1\n",
            "  (0, 120004)\t1\n",
            "  (0, 36077)\t1\n",
            "  (0, 59776)\t1\n",
            "  (0, 107041)\t1\n",
            "  (0, 58398)\t1\n",
            "  (0, 9085)\t1\n",
            "  (0, 48359)\t1\n",
            "  (0, 43097)\t1\n",
            "  (0, 40956)\t1\n",
            "  (0, 40135)\t1\n",
            "  (0, 85110)\t1\n",
            "  (0, 49336)\t1\n",
            "  (0, 45270)\t1\n",
            "  (0, 63341)\t1\n",
            "  (0, 107632)\t1\n",
            "  (0, 120763)\t1\n",
            "  (0, 86979)\t1\n",
            "  (0, 63325)\t1\n",
            "  (0, 63318)\t1\n",
            "  (0, 52385)\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vectorize lines into token sequences attempt 2"
      ],
      "metadata": {
        "id": "iryY5RPsb6Fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# lower = set textx to lowercase\n",
        "# filters are things to be filtered out of text\n",
        "# default filters out punctuation.\n",
        "keras_tokenizer = Tokenizer(lower=True)\n",
        "\n",
        "# train tokenizer\n",
        "keras_tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# tokenize\n",
        "x_train_token = keras_tokenizer.texts_to_sequences(x_train)\n",
        "x_test_token = keras_tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# max length\n",
        "max_length_sentence = 0\n",
        "for sentence in x_train_token:\n",
        "  if len(sentence) > max_length_sentence:\n",
        "    max_length_sentence = len(sentence)\n",
        "\n",
        "# pad vectors: puts 0's at the end\n",
        "x_train_token_pad = pad_sequences(x_train_token, padding = \"post\")\n",
        "\n",
        "# cut any vectors that are too long from testing; pad testing\n",
        "x_test_token_pad = pad_sequences(x_test_token, \n",
        "                                truncating=\"post\", \n",
        "                                maxlen = max_length_sentence, \n",
        "                                padding = \"post\")\n",
        "\n",
        "# find largest value\n",
        "largest_feature_value = 0\n",
        "for sentence in x_train_token_pad:\n",
        "  for word in sentence:\n",
        "    if largest_feature_value < word:\n",
        "      largest_feature_value = word\n"
      ],
      "metadata": {
        "id": "AYlQzX1Zb8t0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Sequential Models"
      ],
      "metadata": {
        "id": "DWdw1tUR37pN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ordinary NN\n"
      ],
      "metadata": {
        "id": "W5GjyN-gZ-rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build model\n",
        "\n",
        "Hyperparameters: \n",
        "* features per layer [first number in the layers.Dense function]\n",
        "* number of layers\n",
        "* activation function\n"
      ],
      "metadata": {
        "id": "JT-k7cYRZ-rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing, Input\n",
        "\n",
        "max_features = max_length_sentence # get feature count from vectorizer\n",
        "max_feature_value = largest_feature_value + 1\n",
        "\n",
        "# sequential model = each layer has 1 input tensor and 1 output tensor\n",
        "model_nn = models.Sequential() \n",
        "# give the model inputs\n",
        "model_nn.add(Input(shape=(max_features,)))\n",
        "# densely connected neural network layer\n",
        "model_nn.add(layers.Dense(100, activation = 'sigmoid'))\n",
        "# densely connected neural network layer\n",
        "model_nn.add(layers.Dense(100, activation = 'relu'))\n",
        "# densely connected neural network layer\n",
        "model_nn.add(layers.Dense(100, activation = 'sigmoid'))\n",
        "# densely connected neural network layer\n",
        "model_nn.add(layers.Dense(1, activation = 'relu'))\n",
        "\n",
        "\n",
        "# compile\n",
        "model_nn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# get summary\n",
        "model_nn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "732e7e7a-c844-4990-d155-d76a95413faa",
        "id": "1nfdz7eDZ-rh"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_28 (Dense)            (None, 100)               23200     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 43,501\n",
            "Trainable params: 43,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "Q7fBohyGZ-rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model_nn.fit(x_train_token_pad, y_train.to_numpy(), epochs=10, batch_size=128, validation_split = .2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8968ab9b-5758-4e04-bc69-322f28eadabb",
        "id": "uVLnRW5qZ-rh"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 2s 9ms/step - loss: 0.6428 - accuracy: 0.7274 - val_loss: 0.5731 - val_accuracy: 0.7546\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5701 - accuracy: 0.7346 - val_loss: 0.6511 - val_accuracy: 0.7546\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5657 - accuracy: 0.7401 - val_loss: 0.9822 - val_accuracy: 0.7546\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5588 - accuracy: 0.7454 - val_loss: 0.7709 - val_accuracy: 0.7546\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5580 - accuracy: 0.7435 - val_loss: 0.5565 - val_accuracy: 0.7534\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5647 - accuracy: 0.7423 - val_loss: 0.5598 - val_accuracy: 0.7493\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5429 - accuracy: 0.7458 - val_loss: 0.9246 - val_accuracy: 0.7546\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5498 - accuracy: 0.7427 - val_loss: 0.6275 - val_accuracy: 0.7528\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 0s 6ms/step - loss: 0.5439 - accuracy: 0.7523 - val_loss: 0.6148 - val_accuracy: 0.7475\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 0s 7ms/step - loss: 0.5428 - accuracy: 0.7476 - val_loss: 1.1297 - val_accuracy: 0.7534\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b3fb9ef70>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "iJeriEJWZ-rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider results\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model_nn.predict(x_test_token_pad)\n",
        "\n",
        "# set cutoff for measurement at 50% for continuous result to binary classification response\n",
        "predictions = [1 if pred >= .5 else 0 for pred in predictions]\n",
        "print(classification_report(y_test.to_numpy(), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29a65078-d684-4cd4-8ca9-34e885f9acf9",
        "id": "BfoOY9cbZ-rh"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114/114 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.00      0.01       935\n",
            "           1       0.74      1.00      0.85      2705\n",
            "\n",
            "    accuracy                           0.74      3640\n",
            "   macro avg       0.71      0.50      0.43      3640\n",
            "weighted avg       0.72      0.74      0.64      3640\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RNN"
      ],
      "metadata": {
        "id": "y41cBjTV4N45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build model"
      ],
      "metadata": {
        "id": "ngw-cRd5PkZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "\n",
        "max_features = max_length_sentence # get feature count from vectorizer\n",
        "max_feature_value = largest_feature_value + 1\n",
        "\n",
        "# sequential model = each layer has 1 input tensor and 1 output tensor\n",
        "model_rnn = models.Sequential() \n",
        "# embedding layer = turns simple input into vectors\n",
        "model_rnn.add(layers.Embedding(max_feature_value, 3, input_length = max_length_sentence))\n",
        "# simplernn = recurrent neural network: has memory/satate to learn sequence\n",
        "# final state is formed from previous states\n",
        "model_rnn.add(layers.SimpleRNN(32))\n",
        "# densely connected neural network layer\n",
        "model_rnn.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "# compile\n",
        "model_rnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# get summary\n",
        "model_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YiXNLmXf5VZ",
        "outputId": "8322c225-c288-47c2-8b74-06c77e587c27"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 231, 3)            62637     \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 32)                1152      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 63,822\n",
            "Trainable params: 63,822\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "qfuATS-dPpl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "# class weight\n",
        "class_0 = 0\n",
        "class_1 = 0\n",
        "for c in y_train:\n",
        "  if c == 0:\n",
        "    class_0 += 1\n",
        "  if c == 1:\n",
        "    class_1 += 1\n",
        "weight = {0: class_0 / (class_0 + class_1), 1:class_1 / (class_0 + class_1)}\n",
        "\n",
        "model_rnn.fit(x_train_token_pad, y_train.to_numpy(), epochs=10, batch_size=128, validation_split = .2, class_weight=weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5T8-PKUPqgg",
        "outputId": "ffb291bd-b561-4ec9-dfe5-c0ebe006a2bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 6s 78ms/step - loss: 0.2810 - accuracy: 0.6922 - val_loss: 0.6744 - val_accuracy: 0.7546\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 4s 73ms/step - loss: 0.2061 - accuracy: 0.7467 - val_loss: 0.6106 - val_accuracy: 0.7546\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 5s 98ms/step - loss: 0.2061 - accuracy: 0.7467 - val_loss: 0.6335 - val_accuracy: 0.7546\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 4s 73ms/step - loss: 0.2059 - accuracy: 0.7467 - val_loss: 0.6515 - val_accuracy: 0.7546\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 4s 71ms/step - loss: 0.2059 - accuracy: 0.7467 - val_loss: 0.6736 - val_accuracy: 0.7546\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 5s 98ms/step - loss: 0.2061 - accuracy: 0.7467 - val_loss: 0.6086 - val_accuracy: 0.7546\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 4s 73ms/step - loss: 0.2062 - accuracy: 0.7467 - val_loss: 0.6033 - val_accuracy: 0.7546\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 4s 71ms/step - loss: 0.2061 - accuracy: 0.7467 - val_loss: 0.6186 - val_accuracy: 0.7546\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 5s 97ms/step - loss: 0.2060 - accuracy: 0.7467 - val_loss: 0.6567 - val_accuracy: 0.7546\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 4s 72ms/step - loss: 0.2060 - accuracy: 0.7467 - val_loss: 0.6646 - val_accuracy: 0.7546\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b43eb0dc0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "giIvI7wuPsk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider results\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model_rnn.predict(x_test_token_pad)\n",
        "\n",
        "# set cutoff for measurement at 50% for continuous result to binary classification response\n",
        "predictions = [1 if pred >= .5 else 0 for pred in predictions]\n",
        "print(classification_report(y_test.to_numpy(), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BI6CGJmePsVa",
        "outputId": "08a09998-e330-45ef-868a-5b9ae5b9ffa1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114/114 [==============================] - 2s 18ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       935\n",
            "           1       0.74      1.00      0.85      2705\n",
            "\n",
            "    accuracy                           0.74      3640\n",
            "   macro avg       0.37      0.50      0.43      3640\n",
            "weighted avg       0.55      0.74      0.63      3640\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN"
      ],
      "metadata": {
        "id": "5ngGldEw4MWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Model\n",
        "\n",
        "Hyperparameters:\n",
        "* embedding size\n",
        "* activation functions in convolutional layers\n",
        "* shape of pooling"
      ],
      "metadata": {
        "id": "uIkzzrVaQIiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models, preprocessing\n",
        "\n",
        "max_features = max_length_sentence # get feature count from vectorizer\n",
        "max_feature_value = largest_feature_value + 1\n",
        "\n",
        "model_cnn = models.Sequential() # 1 input and output per layer\n",
        "model_cnn.add(layers.Embedding(max_feature_value, 50, input_length = max_length_sentence))\n",
        "model_cnn.add(layers.Conv1D(32, 7, activation = 'relu')) # convolutional layer\n",
        "model_cnn.add(layers.MaxPooling1D(5)) # reduce dimensionality\n",
        "model_cnn.add(layers.Conv1D(32, 7, activation = \"relu\"))\n",
        "model_cnn.add(layers.GlobalMaxPooling1D())  # pool together\n",
        "model_cnn.add(layers.Dense(1)) # become 1 output\n",
        "\n",
        "# compile\n",
        "model_cnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# get summary\n",
        "model_cnn.summary()"
      ],
      "metadata": {
        "id": "TBDrIukI4MG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6809b96f-2033-42ce-cd31-341e0525838d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 231, 50)           1043950   \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 225, 32)           11232     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 45, 32)           0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 39, 32)            7200      \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 32)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,062,415\n",
            "Trainable params: 1,062,415\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "bsQj7pLlQKFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train\n",
        "model_cnn.fit(x_train_token_pad, y_train.to_numpy(), epochs=10, batch_size=128, validation_split = .2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZPq_EpzQJyp",
        "outputId": "a42f5357-2ec5-4879-a577-d70ef0a565d7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 9s 143ms/step - loss: 0.8262 - accuracy: 0.6754 - val_loss: 0.6082 - val_accuracy: 0.7540\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 9s 160ms/step - loss: 0.4763 - accuracy: 0.7776 - val_loss: 0.4744 - val_accuracy: 0.8405\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 7s 133ms/step - loss: 0.3011 - accuracy: 0.8996 - val_loss: 0.4409 - val_accuracy: 0.8846\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 14s 250ms/step - loss: 0.1856 - accuracy: 0.9438 - val_loss: 0.2810 - val_accuracy: 0.9058\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 9s 159ms/step - loss: 0.1324 - accuracy: 0.9661 - val_loss: 0.2866 - val_accuracy: 0.9176\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 8s 140ms/step - loss: 0.0938 - accuracy: 0.9795 - val_loss: 1.0997 - val_accuracy: 0.7616\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 8s 140ms/step - loss: 0.0824 - accuracy: 0.9837 - val_loss: 0.4818 - val_accuracy: 0.9217\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 12s 227ms/step - loss: 0.0617 - accuracy: 0.9893 - val_loss: 0.6733 - val_accuracy: 0.8876\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 10s 189ms/step - loss: 0.0655 - accuracy: 0.9913 - val_loss: 0.5267 - val_accuracy: 0.9241\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 10s 182ms/step - loss: 0.0525 - accuracy: 0.9937 - val_loss: 0.5800 - val_accuracy: 0.9276\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2b43bf8610>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test model"
      ],
      "metadata": {
        "id": "UdmDQAa7QKpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# consider results\n",
        "from sklearn.metrics import classification_report\n",
        "predictions = model_cnn.predict(x_test_token_pad)\n",
        "\n",
        "# set cutoff for measurement at 50% for continuous result to binary classification response\n",
        "predictions = [1 if pred >= .5 else 0 for pred in predictions]\n",
        "print(classification_report(y_test.to_numpy(), predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Apn3HPPQKbi",
        "outputId": "b434dd5e-42d9-4a00-b070-27b58c30954c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "114/114 [==============================] - 2s 14ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.86      0.87       935\n",
            "           1       0.95      0.96      0.96      2705\n",
            "\n",
            "    accuracy                           0.93      3640\n",
            "   macro avg       0.92      0.91      0.91      3640\n",
            "weighted avg       0.93      0.93      0.93      3640\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Models\n"
      ],
      "metadata": {
        "id": "2ZCvr2YSMS9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes"
      ],
      "metadata": {
        "id": "smnyioSXMWoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "2TU9ZhkTPho_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "bayes = MultinomialNB()\n",
        "bayes.fit(x_vector_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "0U3QMmhUPhRn",
        "outputId": "36d0f50b-693b-4b62-ed26-63ad9306c136"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test and evaluate model"
      ],
      "metadata": {
        "id": "rvDd1fGVPjnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bayes_pred = bayes.predict(x_vector_test)\n",
        "\n",
        "# print results\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, bayes_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rDKHZeKPlXp",
        "outputId": "dd86f35e-1013-40f2-a5d8-6747dab4e0ee"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.78      0.86       935\n",
            "           1       0.93      0.99      0.96      2705\n",
            "\n",
            "    accuracy                           0.94      3640\n",
            "   macro avg       0.95      0.88      0.91      3640\n",
            "weighted avg       0.94      0.94      0.93      3640\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logicstic Regression"
      ],
      "metadata": {
        "id": "bhNE_S5_MZiT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "b4H-nHcQakut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "regression = LogisticRegression(solver='lbfgs')\n",
        "regression.fit(x_vector_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "9518c9af-c67d-4be3-8177-2bf961856b68",
        "id": "VjE3zyuZakut"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test and evaluate model"
      ],
      "metadata": {
        "id": "4KHDR6sEakut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "regression_pred = regression.predict(x_vector_test)\n",
        "\n",
        "# print results\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, regression_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc42d2c7-8f44-4a25-a097-598e2444fec6",
        "id": "5uFffB4Hakuu"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.78      0.85       935\n",
            "           1       0.93      0.98      0.95      2705\n",
            "\n",
            "    accuracy                           0.93      3640\n",
            "   macro avg       0.92      0.88      0.90      3640\n",
            "weighted avg       0.93      0.93      0.92      3640\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "Ocpm8WaCMbrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model"
      ],
      "metadata": {
        "id": "KJmGZcNLalSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "net = MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=11)\n",
        "net.fit(x_vector_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "0d9ab43f-7d7a-4e6f-c581-6feaab668437",
        "id": "bTS5KmZmalSu"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=11)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=11)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=500, random_state=11)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test and evaluate model"
      ],
      "metadata": {
        "id": "COYswy_MalSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_pred = net.predict(x_vector_test)\n",
        "\n",
        "# print results\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, net_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2754bcef-275f-423f-e23a-1b400b144492",
        "id": "lpXYLSKAalSu"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.84      0.87       935\n",
            "           1       0.95      0.97      0.96      2705\n",
            "\n",
            "    accuracy                           0.94      3640\n",
            "   macro avg       0.92      0.91      0.91      3640\n",
            "weighted avg       0.93      0.94      0.93      3640\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Report\n",
        "\n",
        "This experiment is a continuation of the previous text classification project; review the previous text classification project for the report of methods and traits of the sklearn models.\n",
        "\n",
        "## Preprocessing\n",
        "The model consists of a corpus of sentences from two books, with the goal of classifying a sentence as belonging to one book or the other. Legal text is removed from both raw text sources and the sources are tokenized.\n",
        "\n",
        "The sentences are labeled as to which sentence they came from and then scrambled into a train-test assembly with 33% of the data going into the test set.\n",
        "\n",
        "Both the train set and test sets are tokenized into lowercase words and transformed into a vector.\n",
        "* The 'sentence' vector has an entry for each token in a sentence. Tokens are represented as numbers; only tokens in the training set are given a token id, and two tokens with the same token representation are represented by the same number.\n",
        "* A 'sentence' vector is padded at the end with 0's to conform to the sentence length n, the size of the longest sentence in the training set. Testing sentences longer than n are cut at the nth character.\n",
        "* Punctuation is not considered for tokenization and is discarded.\n",
        "\n",
        "The naive bayes, sklearn implementation of a nerual network, and logistic regression functions all work off a bag of words approach.\n",
        "\n",
        "## Sequential Networks\n",
        "Sequential networks are neural networks that are composed of several layers, where each layer's output feeds the subsequent layer's input.\n",
        "\n",
        "Of the approaches tested, both the ordinary nerual network and the RNN network are very ill-suited to the task of classification when the input is a vector of tokens; they would probably perform better on something like a bag of words representation.\n",
        "\n",
        "CNN, however, was well suited to the task of clssification on an ordered vector of tokens.\n",
        "\n",
        "Compared to the typical size of datasets used for training deep learning networks, this dataset is very small which presents yet another reason that the NN and RNN networks performed very poorly.\n",
        "\n",
        "None of the deep learning tasks outperformed simpler sklearn functions operated on bag of words representations as opposed to a vector of tokens, though CNN yielded performance that was comparable.\n",
        "\n",
        "### NN\n",
        "An ordinary neural network learns a pattern from adjusting weights so the answer it predicts for an example more closely matches the actual example class.\n",
        "\n",
        "* Variations can be made by using more or fewer layers, changing the number of nodes in a layer, changing the activation function, and using techniques like early cutoff training.\n",
        "* In experiments run for this project, 'sigmoid' activation performed better than 'relu' (higher weighted accuracy and F1-score) for densely connected layers of 100 nodes.\n",
        "* Using a deeper network (more layers) improved performance over only 1 layer of internal nodes.\n",
        "* Increasing the number of nodes only sometimes improved performance, with both 1000 and 500 nodes in each layer negatively impacting performance over 100 nodes.\n",
        "* A poor selection of layer node numbers can easily classify all examples as one classification or the other with binary classification, resulting in very poor accuracy results.\n",
        "\n",
        "### CNN\n",
        "A convolutional neural network learns patterns in data from small windows called filters. This type of neural network is better for recognizing relationships between features with some independence of the precise positioning of the features in the input sequence. These networks work very well for image data since images are collections of patterns whose meaning is independent of their spatial location in a matrix. For this reason, they may also be good at detecting grammatical devices if the sentence style differs significantly between the two different books being classified.\n",
        "* 1D convolutional layers are used since they best represent sequential data. (2D would be useful for images, and 3D for volume spaces)\n",
        "* On this data, there is only a marginal difference between an embedding size of 10 and 128, but a size of 1000 is wildly inefficient, failing to classify one type of document entirelly. 50 ended up having greater accuracy than the 10 and 128. Since a number of elements more reasonable to the size of a sentence seemed to improve performance, the size of 20 was tested; this one had radically poor performance due to misclassifying an entire category. A probable cause for this is that the models were trained to value accuracy instead of a more wholistic metric like F-1 score.\n",
        "* The CNN models significantly outperformed the tensorflow NN model.\n",
        "\n",
        "\n",
        "### RNN\n",
        "A recurrent neural network uses a state to learn a sequence. This model encodes data of patterns in features it has observed while examining an example of data.* Several variations exist to address problems like the vanishing gradient problem; the nuances of these models is not explored in this report.\n",
        "* I could not find an arrangement of values where the RNN did not completely misclassify one of the classes. No embedding size fixed the issue, and changing the weights of the classes to more evenly distribute training examples also did not change the results.\n",
        "* RNN models appear to be very poorly suited for this task with the current feature tokenization."
      ],
      "metadata": {
        "id": "wtu76hWydYg7"
      }
    }
  ]
}
